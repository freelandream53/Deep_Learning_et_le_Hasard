{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cas de prédiction du Loto français "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des librairies utiles\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de scraping des tirages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de scraping des tirages du loto\n",
    "df_tirage = pd.read_csv('kenodata.csv', sep = ',', usecols=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9','num10','num11','num12','num13','num14','num15','num16','num17','num18','num19'])\n",
    "dtype={'num':str }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A noter que plusieurs tirages se sont ajoutés dépuis le 21 : date de rédaction de l'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num0</th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>num11</th>\n",
       "      <th>num12</th>\n",
       "      <th>num13</th>\n",
       "      <th>num14</th>\n",
       "      <th>num15</th>\n",
       "      <th>num16</th>\n",
       "      <th>num17</th>\n",
       "      <th>num18</th>\n",
       "      <th>num19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num0  num1  num2  num3  num4  num5  num6  num7  num8  num9  num10  num11  \\\n",
       "0     2     3     6     7    11    13    14    15    19    20     27     28   \n",
       "1     2     6    15    20    24    31    32    34    35    36     37     39   \n",
       "2     2     9    19    20    21    23    25    26    29    33     34     36   \n",
       "3     3     4     6    13    14    16    19    21    22    26     33     34   \n",
       "4     4     6    14    16    17    19    20    29    31    34     38     39   \n",
       "\n",
       "   num12  num13  num14  num15  num16  num17  num18  num19  \n",
       "0     29     32     37     38     43     48     54     60  \n",
       "1     41     42     44     45     52     61     62     65  \n",
       "2     37     40     43     45     47     51     53     68  \n",
       "3     35     36     38     52     55     64     65     69  \n",
       "4     44     50     53     56     57     62     64     67  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sracping des tirages actuellement disponibles sur le site \n",
    "\n",
    "df_tirage[['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9','num10','num11','num12','num13','num14','num15','num16','num17','num18','num19']].head()\n",
    "#suppression  des tirages du super loto( A explorer later )\n",
    "#df_tirage=df_tirage[(df_tirage['day']!='Vendredi') & (df_tirage['day']!='Mardi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tirage=df_tirage.tail(df_tirage.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commentaires: \n",
    "* le dernier tirage ici date du 07 décembre, ainsi afin de tester le modèle nous allons rétirer ce tirage du dataset dans la suite\n",
    "* Par contre on aurait évité de supprimer le tirage du 28 si on voulait prédire le prochain tirage ( celui du samedi 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num0</th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>num11</th>\n",
       "      <th>num12</th>\n",
       "      <th>num13</th>\n",
       "      <th>num14</th>\n",
       "      <th>num15</th>\n",
       "      <th>num16</th>\n",
       "      <th>num17</th>\n",
       "      <th>num18</th>\n",
       "      <th>num19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num0  num1  num2  num3  num4  num5  num6  num7  num8  num9  num10  num11  \\\n",
       "0     2     3     6     7    11    13    14    15    19    20     27     28   \n",
       "1     2     6    15    20    24    31    32    34    35    36     37     39   \n",
       "2     2     9    19    20    21    23    25    26    29    33     34     36   \n",
       "3     3     4     6    13    14    16    19    21    22    26     33     34   \n",
       "4     4     6    14    16    17    19    20    29    31    34     38     39   \n",
       "\n",
       "   num12  num13  num14  num15  num16  num17  num18  num19  \n",
       "0     29     32     37     38     43     48     54     60  \n",
       "1     41     42     44     45     52     61     62     65  \n",
       "2     37     40     43     45     47     51     53     68  \n",
       "3     35     36     38     52     55     64     65     69  \n",
       "4     44     50     53     56     57     62     64     67  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_tirage=df_tirage.tail(df_tirage.shape[0])# suppression du dernier tirage/à éviter selon le cas \n",
    "df_tirage.head()# le dernier tirage devient ici celui du 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement  des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tirage.iloc[::-1]#inversion du dataframe pour placer le dernier tirage en dernière position\n",
    "df = df[['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9','num10','num11','num12','num13','num14','num15','num16','num17','num18','num19']]\n",
    "#sélection des numéros à  traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num0</th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>num11</th>\n",
       "      <th>num12</th>\n",
       "      <th>num13</th>\n",
       "      <th>num14</th>\n",
       "      <th>num15</th>\n",
       "      <th>num16</th>\n",
       "      <th>num17</th>\n",
       "      <th>num18</th>\n",
       "      <th>num19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num0  num1  num2  num3  num4  num5  num6  num7  num8  num9  num10  num11  \\\n",
       "4     4     6    14    16    17    19    20    29    31    34     38     39   \n",
       "3     3     4     6    13    14    16    19    21    22    26     33     34   \n",
       "2     2     9    19    20    21    23    25    26    29    33     34     36   \n",
       "1     2     6    15    20    24    31    32    34    35    36     37     39   \n",
       "0     2     3     6     7    11    13    14    15    19    20     27     28   \n",
       "\n",
       "   num12  num13  num14  num15  num16  num17  num18  num19  \n",
       "4     44     50     53     56     57     62     64     67  \n",
       "3     35     36     38     52     55     64     65     69  \n",
       "2     37     40     43     45     47     51     53     68  \n",
       "1     41     42     44     45     52     61     62     65  \n",
       "0     29     32     37     38     43     48     54     60  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()# notre tirage du 26 ici devient le dernier de notre dataset afin de pourvoir organiser les data par historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#fonction de vérification de nombres en dessous d'une certaine valeur pour les 5 premiers numéros, sauf celui de chance\n",
    "def is_under(data, number):\n",
    "    return ((data['num0'] <= number).astype(int) + \n",
    "            (data['num1'] <= number).astype(int) +\n",
    "            (data['num2'] <= number).astype(int) +\n",
    "            (data['num3'] <= number).astype(int) +\n",
    "            (data['num4'] <= number).astype(int) +\n",
    "            (data['num5'] <= number).astype(int) + \n",
    "            (data['num6'] <= number).astype(int) +\n",
    "            (data['num7'] <= number).astype(int) +\n",
    "            (data['num8'] <= number).astype(int) +\n",
    "            (data['num9'] <= number).astype(int) +\n",
    "            (data['num10'] <= number).astype(int) + \n",
    "            (data['num11'] <= number).astype(int) +\n",
    "            (data['num12'] <= number).astype(int) +\n",
    "            (data['num13'] <= number).astype(int) +\n",
    "            (data['num14'] <= number).astype(int) +\n",
    "            (data['num15'] <= number).astype(int) + \n",
    "            (data['num16'] <= number).astype(int) +\n",
    "            (data['num17'] <= number).astype(int) +\n",
    "            (data['num18'] <= number).astype(int) +\n",
    "            (data['num19'] <= number).astype(int))\n",
    "\n",
    "#fonction de vérification de nombres pairs pour les 5 premiers numéros sauf celui de chance\n",
    "def is_pair(data):\n",
    "    return ((data['num0'].isin(pairs)).astype(int) + \n",
    "            (data['num1'].isin(pairs)).astype(int) +\n",
    "            (data['num2'].isin(pairs)).astype(int) +\n",
    "            (data['num3'].isin(pairs)).astype(int) +\n",
    "            (data['num4'].isin(pairs)).astype(int) +\n",
    "            (data['num5'].isin(pairs)).astype(int) + \n",
    "            (data['num6'].isin(pairs)).astype(int) +\n",
    "            (data['num7'].isin(pairs)).astype(int) +\n",
    "            (data['num8'].isin(pairs)).astype(int) +\n",
    "            (data['num9'].isin(pairs)).astype(int) +\n",
    "            (data['num10'].isin(pairs)).astype(int) + \n",
    "            (data['num11'].isin(pairs)).astype(int) +\n",
    "            (data['num12'].isin(pairs)).astype(int) +\n",
    "            (data['num13'].isin(pairs)).astype(int) +\n",
    "            (data['num14'].isin(pairs)).astype(int) +\n",
    "            (data['num15'].isin(pairs)).astype(int) + \n",
    "            (data['num16'].isin(pairs)).astype(int) +\n",
    "            (data['num17'].isin(pairs)).astype(int) +\n",
    "            (data['num18'].isin(pairs)).astype(int) +\n",
    "            (data['num19'].isin(pairs)).astype(int))\n",
    "\n",
    "#fonction de vérification de nombres impairs pour les 5 premiers numéros sauf celui de chance\n",
    "def is_impair(data):\n",
    "    return ((data['num0'].isin(impairs)).astype(int) + \n",
    "            (data['num1'].isin(impairs)).astype(int) +\n",
    "            (data['num2'].isin(impairs)).astype(int) +\n",
    "            (data['num3'].isin(impairs)).astype(int) +\n",
    "            (data['num4'].isin(impairs)).astype(int) +\n",
    "            (data['num5'].isin(impairs)).astype(int) + \n",
    "            (data['num6'].isin(impairs)).astype(int) +\n",
    "            (data['num7'].isin(impairs)).astype(int) +\n",
    "            (data['num8'].isin(impairs)).astype(int) +\n",
    "            (data['num9'].isin(impairs)).astype(int) +\n",
    "            (data['num10'].isin(impairs)).astype(int) + \n",
    "            (data['num11'].isin(impairs)).astype(int) +\n",
    "            (data['num12'].isin(impairs)).astype(int) +\n",
    "            (data['num13'].isin(impairs)).astype(int) +\n",
    "            (data['num14'].isin(impairs)).astype(int) +\n",
    "            (data['num15'].isin(impairs)).astype(int) + \n",
    "            (data['num16'].isin(impairs)).astype(int) +\n",
    "            (data['num17'].isin(impairs)).astype(int) +\n",
    "            (data['num18'].isin(impairs)).astype(int) +\n",
    "            (data['num19'].isin(impairs)).astype(int))\n",
    "\n",
    "\n",
    "\n",
    "#liste de nombres pairs et impairs\n",
    "pairs = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70]\n",
    "impairs = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69]\n",
    "\n",
    "#Fonction de calcul de la somme de la différence au carré des 5 premiers numéros, sauf celui de chance\n",
    "def sum_diff(data):\n",
    "    return ((data['num1'] - data['num0'])**2 + \n",
    "            (data['num2'] - data['num1'])**2 +\n",
    "            (data['num3'] - data['num2'])**2 +\n",
    "            (data['num4'] - data['num3'])**2 +\n",
    "            (data['num5'] - data['num4'])**2 + \n",
    "            (data['num6'] - data['num5'])**2 +\n",
    "            (data['num7'] - data['num6'])**2 +\n",
    "            (data['num8'] - data['num7'])**2 +\n",
    "            (data['num9'] - data['num8'])**2 + \n",
    "            (data['num10'] - data['num9'])**2 +\n",
    "            (data['num11'] - data['num10'])**2 +\n",
    "            (data['num12'] - data['num11'])**2 +\n",
    "            (data['num13'] - data['num12'])**2 + \n",
    "            (data['num14'] - data['num13'])**2 +\n",
    "            (data['num15'] - data['num14'])**2 +\n",
    "            (data['num16'] - data['num15'])**2 +\n",
    "            (data['num17'] - data['num16'])**2 + \n",
    "            (data['num18'] - data['num17'])**2 +\n",
    "            (data['num19'] - data['num18'])**2)\n",
    "\n",
    "# Calcul de la fréquence de tirage de chaque numéro\n",
    "freqs = []\n",
    "for val in range(70):\n",
    "    count = ( (df['num0'] == val+1).sum() +\n",
    "              (df['num1'] == val+1).sum() +\n",
    "              (df['num2'] == val+1).sum() +\n",
    "              (df['num3'] == val+1).sum() +\n",
    "              (df['num4'] == val+1).sum() +\n",
    "              (df['num5'] == val+1).sum() +\n",
    "              (df['num6'] == val+1).sum() +\n",
    "              (df['num7'] == val+1).sum() +\n",
    "              (df['num8'] == val+1).sum() +\n",
    "              (df['num9'] == val+1).sum() +\n",
    "              (df['num10'] == val+1).sum() +\n",
    "              (df['num11'] == val+1).sum() +\n",
    "              (df['num12'] == val+1).sum() +\n",
    "              (df['num13'] == val+1).sum() +\n",
    "              (df['num14'] == val+1).sum() +\n",
    "              (df['num15'] == val+1).sum() +\n",
    "              (df['num16'] == val+1).sum() +\n",
    "              (df['num17'] == val+1).sum() +\n",
    "              (df['num18'] == val+1).sum() +\n",
    "              (df['num19'] == val+1).sum() )\n",
    "    \n",
    "    freqs.append(count)\n",
    "ax = plt.gca() ;  ax.invert_yaxis()\n",
    "plt.gcf().set_size_inches(7, 6)\n",
    "heatmap = plt.pcolor(np.reshape(np.array(freqs), (7, 10)), cmap=plt.cm.Blues)\n",
    "\n",
    "def freq_val(data, column):\n",
    "    tab = data[column].values.tolist()\n",
    "    freqs = []\n",
    "    pos = 1\n",
    "    for e in tab:\n",
    "        freqs.append(tab[0:pos].count(e))\n",
    "        pos = pos + 1\n",
    "    return freqs\n",
    "\n",
    "\n",
    "\n",
    "df['sum'] = ((df.num0 + df.num1 + df.num2 + df.num3 + df.num4 + df.num5 + df.num6 + df.num7 + df.num8 + df.num9 +df.num10 + df.num11 + df.num12 + df.num13 + df.num14 +df.num15 + df.num16 + df.num17 + df.num18 + df.num19 ) >1400).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num0</th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_num15</th>\n",
       "      <th>freq_num16</th>\n",
       "      <th>freq_num17</th>\n",
       "      <th>freq_num18</th>\n",
       "      <th>freq_num19</th>\n",
       "      <th>sum_diff</th>\n",
       "      <th>pair</th>\n",
       "      <th>impair</th>\n",
       "      <th>is_under_24</th>\n",
       "      <th>is_under_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>368</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>360</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>393</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num0  num1  num2  num3  num4  num5  num6  num7  num8  num9  ...  \\\n",
       "1251     2     7    12    17    19    21    25    29    36    37  ...   \n",
       "1250     5     8    10    15    16    25    28    29    31    32  ...   \n",
       "1249     5    16    17    18    22    27    30    31    32    35  ...   \n",
       "1248     1     2     4     5    11    12    16    19    21    22  ...   \n",
       "1247     2     4     7     8    14    16    18    23    31    38  ...   \n",
       "1246     1     3     8    10    15    16    18    21    22    23  ...   \n",
       "\n",
       "      freq_num15  freq_num16  freq_num17  freq_num18  freq_num19  sum_diff  \\\n",
       "1251           1           1           1           1           1       293   \n",
       "1250           1           2           1           2           1       341   \n",
       "1249           1           1           1           1           2       368   \n",
       "1248           1           1           1           3           3       360   \n",
       "1247           1           2           1           1           1       312   \n",
       "1246           1           1           1           2           2       393   \n",
       "\n",
       "      pair  impair  is_under_24  is_under_48  \n",
       "1251     7      13            6           13  \n",
       "1250    10      10            5           14  \n",
       "1249     8      12            5           14  \n",
       "1248     9      11           11           14  \n",
       "1247    13       7            8           15  \n",
       "1246     9      11           10           16  \n",
       "\n",
       "[6 rows x 47 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajout de la difference entre les numéros(A explorer ASAp)\n",
    "for i in range(4):\n",
    "    (i,i+1)\n",
    "df['diff_{}'.format(i)]=df['num{}'.format(i+1)]-df['num{}'.format(i)]\n",
    "#application des fonctions sur le dataframe\n",
    "df['freq_num0'] = freq_val(df, 'num0')\n",
    "df['freq_num1'] = freq_val(df, 'num1')\n",
    "df['freq_num2'] = freq_val(df, 'num2')\n",
    "df['freq_num3'] = freq_val(df, 'num3')\n",
    "df['freq_num4'] = freq_val(df, 'num4')\n",
    "df['freq_num5'] = freq_val(df, 'num5')\n",
    "df['freq_num6'] = freq_val(df, 'num6')\n",
    "df['freq_num7'] = freq_val(df, 'num7')\n",
    "df['freq_num8'] = freq_val(df, 'num8')\n",
    "df['freq_num9'] = freq_val(df, 'num9')\n",
    "df['freq_num10'] = freq_val(df, 'num10')\n",
    "df['freq_num11'] = freq_val(df, 'num11')\n",
    "df['freq_num12'] = freq_val(df, 'num12')\n",
    "df['freq_num13'] = freq_val(df, 'num13')\n",
    "df['freq_num14'] = freq_val(df, 'num14')\n",
    "df['freq_num15'] = freq_val(df, 'num15')\n",
    "df['freq_num16'] = freq_val(df, 'num16')\n",
    "df['freq_num17'] = freq_val(df, 'num17')\n",
    "df['freq_num18'] = freq_val(df, 'num18')\n",
    "df['freq_num19'] = freq_val(df, 'num19')\n",
    "df['sum_diff'] = sum_diff(df)#somme de la différence au carré entre chaque couple de numéros successifs dans le tirage\n",
    "df['pair'] = is_pair(df)\n",
    "df['impair'] = is_impair(df)#verification de nombre pair et impair\n",
    "df['is_under_24'] = is_under(df, 24)  # Les numeros en dessous de 24 \n",
    "df['is_under_48'] = is_under(df, 48)# Les numeros en dessous de 40 \n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle et fonction de formatage des données en entrée du LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture 3: fonction define model seulement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j'ai ici défini plusieurs modèles à tester mais pour l'intant je tavaille avec le lstm(fonction : define_model)\n",
    "# j'ai ici défini window_length à 12 pour apprendre sur 1 mois de données \n",
    "\n",
    "#Params du modèle\n",
    "nb_label_feature=20\n",
    "\n",
    "UNITS = 250\n",
    "BATCHSIZE = 45\n",
    "EPOCH = 1500\n",
    "#ACTIVATION = \"softmax\"\n",
    "OPTIMIZER ='adam' # rmsprop, adam, sgd\n",
    "LOSS = 'mae'#'categorical_crossentropy' #mse\n",
    "DROPOUT = 0.11\n",
    "window_length =23 #12 \n",
    "number_of_features = df.shape[1]\n",
    "\n",
    "#Architecture du modèle\n",
    "def define_model(number_of_features,nb_label_feature):\n",
    "    #initialisation du rnn\n",
    "    model = Sequential()\n",
    "    #ajout de la premiere couche lstm\n",
    "    model.add(LSTM(UNITS, input_shape=(window_length, number_of_features), return_sequences=True))\n",
    "    model.add(LSTM(UNITS, dropout=0.1, return_sequences=False))\n",
    "    #ajout de la couche de sortie\n",
    "    model.add(Dense(nb_label_feature))\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def define_bidirectionnel_model(number_of_features,nb_label_feature):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(100, dropout=0.2, return_sequences=True), input_shape=(window_length, number_of_features)))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(100, dropout=0.1))\n",
    "    model.add(Dense(nb_label_feature))\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def define_autoencoder_model(number_of_features,nb_label_feature):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(window_length, number_of_features), return_sequences=True))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(RepeatVector(window_length))\n",
    "    model.add(LSTM(100, dropout=0.1, return_sequences=True))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(number_of_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_label_feature))\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#model = define_model(number_of_features,nb_label_feature)\n",
    "#model3 = define_autoencoder_model(number_of_features,nb_label_feature)\n",
    "#model4 = define_bidirectionnel_model(number_of_features,nb_label_feature)\n",
    "\n",
    "#Moniteur pour stoper le training\n",
    "es = EarlyStopping(monitor='acc', mode='max', verbose=1, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de formatage des données en entrée du LSTM\n",
    "def create_lstm_dataset(df, window_length,nb_label_feature):\n",
    "    number_of_rows = df.shape[0]   #taille du dataset number_of_features\n",
    "    number_of_features = df.shape[1]\n",
    "    scaler = StandardScaler().fit(df.values)\n",
    "    transformed_dataset = scaler.transform(df.values)\n",
    "    transformed_df = pd.DataFrame(data=transformed_dataset, index=df.index)\n",
    "    #tableau de tableau de taille(number_of_rows-window_length) et window_length ligne,number_of_features\n",
    "    #lstm:[nb total de row ,nb de ligne dans le passé, nb de colonne(feature)]\n",
    "    train = np.empty([number_of_rows-window_length, window_length, number_of_features], dtype=float)\n",
    "    \n",
    "    label = np.empty([number_of_rows-window_length, nb_label_feature], dtype=float)\n",
    "    for i in range(0, number_of_rows-window_length):\n",
    "        train[i] = transformed_df.iloc[i:i+window_length, 0: number_of_features]\n",
    "        label[i] = transformed_df.iloc[i+window_length: i+window_length+1, 0:nb_label_feature]\n",
    "        \n",
    "    #définition du modèle Lstm  \n",
    "    model = define_model(number_of_features,nb_label_feature)\n",
    "        \n",
    "    return train, label, model,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1229, 23, 47)\n",
      "(1229, 20)\n"
     ]
    }
   ],
   "source": [
    "#formatage des données\n",
    "train, label,model,scaler1 = create_lstm_dataset(df, window_length,nb_label_feature)\n",
    "print(train.shape)\n",
    "print(label.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On voit ici que notre dataset d'entrainement après formatage est constitué de 1911 vecteurs contenant chacun 12 tirages où chaque tirage contient 19 features calculés plus haut\n",
    "\n",
    "* Quant aux labels, on a bien 1911 vecteurs de 6 features soit les 6 numéros de chaque tirages\n",
    "\n",
    "* Ainsi à partir des 12 tirages précédent on éssaie de prédire le tirage suivant lors de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "28/28 - 6s - loss: 0.8010 - acc: 0.0635 - 6s/epoch - 203ms/step\n",
      "Epoch 2/1500\n",
      "28/28 - 2s - loss: 0.7918 - acc: 0.0789 - 2s/epoch - 83ms/step\n",
      "Epoch 3/1500\n",
      "28/28 - 2s - loss: 0.7874 - acc: 0.0862 - 2s/epoch - 81ms/step\n",
      "Epoch 4/1500\n",
      "28/28 - 3s - loss: 0.7832 - acc: 0.0903 - 3s/epoch - 90ms/step\n",
      "Epoch 5/1500\n",
      "28/28 - 2s - loss: 0.7775 - acc: 0.0862 - 2s/epoch - 80ms/step\n",
      "Epoch 6/1500\n",
      "28/28 - 3s - loss: 0.7755 - acc: 0.0773 - 3s/epoch - 91ms/step\n",
      "Epoch 7/1500\n",
      "28/28 - 2s - loss: 0.7647 - acc: 0.0854 - 2s/epoch - 82ms/step\n",
      "Epoch 8/1500\n",
      "28/28 - 2s - loss: 0.7598 - acc: 0.0960 - 2s/epoch - 79ms/step\n",
      "Epoch 9/1500\n",
      "28/28 - 2s - loss: 0.7520 - acc: 0.0911 - 2s/epoch - 79ms/step\n",
      "Epoch 10/1500\n",
      "28/28 - 2s - loss: 0.7386 - acc: 0.1050 - 2s/epoch - 79ms/step\n",
      "Epoch 11/1500\n",
      "28/28 - 2s - loss: 0.7232 - acc: 0.1050 - 2s/epoch - 81ms/step\n",
      "Epoch 12/1500\n",
      "28/28 - 2s - loss: 0.7137 - acc: 0.1058 - 2s/epoch - 79ms/step\n",
      "Epoch 13/1500\n",
      "28/28 - 2s - loss: 0.6962 - acc: 0.1155 - 2s/epoch - 81ms/step\n",
      "Epoch 14/1500\n",
      "28/28 - 2s - loss: 0.6878 - acc: 0.1009 - 2s/epoch - 78ms/step\n",
      "Epoch 15/1500\n",
      "28/28 - 2s - loss: 0.6686 - acc: 0.1269 - 2s/epoch - 79ms/step\n",
      "Epoch 16/1500\n",
      "28/28 - 2s - loss: 0.6583 - acc: 0.1383 - 2s/epoch - 79ms/step\n",
      "Epoch 17/1500\n",
      "28/28 - 2s - loss: 0.6329 - acc: 0.1310 - 2s/epoch - 78ms/step\n",
      "Epoch 18/1500\n",
      "28/28 - 2s - loss: 0.6255 - acc: 0.1448 - 2s/epoch - 83ms/step\n",
      "Epoch 19/1500\n",
      "28/28 - 2s - loss: 0.6128 - acc: 0.1741 - 2s/epoch - 78ms/step\n",
      "Epoch 20/1500\n",
      "28/28 - 2s - loss: 0.5890 - acc: 0.1587 - 2s/epoch - 82ms/step\n",
      "Epoch 21/1500\n",
      "28/28 - 2s - loss: 0.5717 - acc: 0.1627 - 2s/epoch - 82ms/step\n",
      "Epoch 22/1500\n",
      "28/28 - 2s - loss: 0.5555 - acc: 0.1814 - 2s/epoch - 79ms/step\n",
      "Epoch 23/1500\n",
      "28/28 - 2s - loss: 0.5366 - acc: 0.1888 - 2s/epoch - 77ms/step\n",
      "Epoch 24/1500\n",
      "28/28 - 2s - loss: 0.5198 - acc: 0.1937 - 2s/epoch - 80ms/step\n",
      "Epoch 25/1500\n",
      "28/28 - 2s - loss: 0.5060 - acc: 0.2124 - 2s/epoch - 82ms/step\n",
      "Epoch 26/1500\n",
      "28/28 - 2s - loss: 0.4858 - acc: 0.2205 - 2s/epoch - 79ms/step\n",
      "Epoch 27/1500\n",
      "28/28 - 2s - loss: 0.4717 - acc: 0.2229 - 2s/epoch - 79ms/step\n",
      "Epoch 28/1500\n",
      "28/28 - 2s - loss: 0.4548 - acc: 0.2506 - 2s/epoch - 79ms/step\n",
      "Epoch 29/1500\n",
      "28/28 - 2s - loss: 0.4343 - acc: 0.2514 - 2s/epoch - 79ms/step\n",
      "Epoch 30/1500\n",
      "28/28 - 2s - loss: 0.4207 - acc: 0.2823 - 2s/epoch - 79ms/step\n",
      "Epoch 31/1500\n",
      "28/28 - 2s - loss: 0.4055 - acc: 0.2750 - 2s/epoch - 78ms/step\n",
      "Epoch 32/1500\n",
      "28/28 - 2s - loss: 0.3913 - acc: 0.2905 - 2s/epoch - 79ms/step\n",
      "Epoch 33/1500\n",
      "28/28 - 2s - loss: 0.3809 - acc: 0.2994 - 2s/epoch - 79ms/step\n",
      "Epoch 34/1500\n",
      "28/28 - 2s - loss: 0.3652 - acc: 0.3084 - 2s/epoch - 76ms/step\n",
      "Epoch 35/1500\n",
      "28/28 - 2s - loss: 0.3497 - acc: 0.3295 - 2s/epoch - 77ms/step\n",
      "Epoch 36/1500\n",
      "28/28 - 2s - loss: 0.3432 - acc: 0.3295 - 2s/epoch - 78ms/step\n",
      "Epoch 37/1500\n",
      "28/28 - 2s - loss: 0.3375 - acc: 0.3458 - 2s/epoch - 79ms/step\n",
      "Epoch 38/1500\n",
      "28/28 - 2s - loss: 0.3248 - acc: 0.3393 - 2s/epoch - 78ms/step\n",
      "Epoch 39/1500\n",
      "28/28 - 2s - loss: 0.3100 - acc: 0.3670 - 2s/epoch - 79ms/step\n",
      "Epoch 40/1500\n",
      "28/28 - 2s - loss: 0.3036 - acc: 0.3824 - 2s/epoch - 77ms/step\n",
      "Epoch 41/1500\n",
      "28/28 - 2s - loss: 0.2931 - acc: 0.3889 - 2s/epoch - 80ms/step\n",
      "Epoch 42/1500\n",
      "28/28 - 2s - loss: 0.2846 - acc: 0.3979 - 2s/epoch - 80ms/step\n",
      "Epoch 43/1500\n",
      "28/28 - 2s - loss: 0.2774 - acc: 0.3914 - 2s/epoch - 78ms/step\n",
      "Epoch 44/1500\n",
      "28/28 - 2s - loss: 0.2724 - acc: 0.4207 - 2s/epoch - 81ms/step\n",
      "Epoch 45/1500\n",
      "28/28 - 2s - loss: 0.2615 - acc: 0.4451 - 2s/epoch - 81ms/step\n",
      "Epoch 46/1500\n",
      "28/28 - 2s - loss: 0.2522 - acc: 0.4540 - 2s/epoch - 79ms/step\n",
      "Epoch 47/1500\n",
      "28/28 - 2s - loss: 0.2476 - acc: 0.4638 - 2s/epoch - 80ms/step\n",
      "Epoch 48/1500\n",
      "28/28 - 2s - loss: 0.2417 - acc: 0.4825 - 2s/epoch - 79ms/step\n",
      "Epoch 49/1500\n",
      "28/28 - 2s - loss: 0.2342 - acc: 0.4703 - 2s/epoch - 81ms/step\n",
      "Epoch 50/1500\n",
      "28/28 - 2s - loss: 0.2314 - acc: 0.4988 - 2s/epoch - 81ms/step\n",
      "Epoch 51/1500\n",
      "28/28 - 2s - loss: 0.2250 - acc: 0.5102 - 2s/epoch - 80ms/step\n",
      "Epoch 52/1500\n",
      "28/28 - 2s - loss: 0.2182 - acc: 0.5159 - 2s/epoch - 79ms/step\n",
      "Epoch 53/1500\n",
      "28/28 - 2s - loss: 0.2131 - acc: 0.5191 - 2s/epoch - 80ms/step\n",
      "Epoch 54/1500\n",
      "28/28 - 2s - loss: 0.2113 - acc: 0.5427 - 2s/epoch - 78ms/step\n",
      "Epoch 55/1500\n",
      "28/28 - 2s - loss: 0.2044 - acc: 0.5427 - 2s/epoch - 79ms/step\n",
      "Epoch 56/1500\n",
      "28/28 - 2s - loss: 0.2000 - acc: 0.5476 - 2s/epoch - 80ms/step\n",
      "Epoch 57/1500\n",
      "28/28 - 2s - loss: 0.1969 - acc: 0.5509 - 2s/epoch - 81ms/step\n",
      "Epoch 58/1500\n",
      "28/28 - 2s - loss: 0.1952 - acc: 0.5761 - 2s/epoch - 80ms/step\n",
      "Epoch 59/1500\n",
      "28/28 - 2s - loss: 0.1901 - acc: 0.5712 - 2s/epoch - 77ms/step\n",
      "Epoch 60/1500\n",
      "28/28 - 2s - loss: 0.1886 - acc: 0.5883 - 2s/epoch - 77ms/step\n",
      "Epoch 61/1500\n",
      "28/28 - 2s - loss: 0.1840 - acc: 0.5883 - 2s/epoch - 79ms/step\n",
      "Epoch 62/1500\n",
      "28/28 - 2s - loss: 0.1803 - acc: 0.6013 - 2s/epoch - 79ms/step\n",
      "Epoch 63/1500\n",
      "28/28 - 2s - loss: 0.1720 - acc: 0.6005 - 2s/epoch - 78ms/step\n",
      "Epoch 64/1500\n",
      "28/28 - 2s - loss: 0.1736 - acc: 0.6208 - 2s/epoch - 78ms/step\n",
      "Epoch 65/1500\n",
      "28/28 - 2s - loss: 0.1700 - acc: 0.6192 - 2s/epoch - 77ms/step\n",
      "Epoch 66/1500\n",
      "28/28 - 2s - loss: 0.1689 - acc: 0.6225 - 2s/epoch - 80ms/step\n",
      "Epoch 67/1500\n",
      "28/28 - 2s - loss: 0.1643 - acc: 0.6477 - 2s/epoch - 79ms/step\n",
      "Epoch 68/1500\n",
      "28/28 - 2s - loss: 0.1615 - acc: 0.6672 - 2s/epoch - 78ms/step\n",
      "Epoch 69/1500\n",
      "28/28 - 2s - loss: 0.1606 - acc: 0.6444 - 2s/epoch - 77ms/step\n",
      "Epoch 70/1500\n",
      "28/28 - 2s - loss: 0.1580 - acc: 0.6574 - 2s/epoch - 79ms/step\n",
      "Epoch 71/1500\n",
      "28/28 - 2s - loss: 0.1554 - acc: 0.6729 - 2s/epoch - 82ms/step\n",
      "Epoch 72/1500\n",
      "28/28 - 2s - loss: 0.1548 - acc: 0.6705 - 2s/epoch - 78ms/step\n",
      "Epoch 73/1500\n",
      "28/28 - 2s - loss: 0.1497 - acc: 0.6851 - 2s/epoch - 78ms/step\n",
      "Epoch 74/1500\n",
      "28/28 - 2s - loss: 0.1493 - acc: 0.6819 - 2s/epoch - 76ms/step\n",
      "Epoch 75/1500\n",
      "28/28 - 2s - loss: 0.1453 - acc: 0.6957 - 2s/epoch - 79ms/step\n",
      "Epoch 76/1500\n",
      "28/28 - 2s - loss: 0.1435 - acc: 0.6973 - 2s/epoch - 77ms/step\n",
      "Epoch 77/1500\n",
      "28/28 - 2s - loss: 0.1451 - acc: 0.6908 - 2s/epoch - 78ms/step\n",
      "Epoch 78/1500\n",
      "28/28 - 2s - loss: 0.1422 - acc: 0.7217 - 2s/epoch - 79ms/step\n",
      "Epoch 79/1500\n",
      "28/28 - 2s - loss: 0.1397 - acc: 0.7046 - 2s/epoch - 79ms/step\n",
      "Epoch 80/1500\n",
      "28/28 - 2s - loss: 0.1418 - acc: 0.7242 - 2s/epoch - 79ms/step\n",
      "Epoch 81/1500\n",
      "28/28 - 2s - loss: 0.1361 - acc: 0.7022 - 2s/epoch - 78ms/step\n",
      "Epoch 82/1500\n",
      "28/28 - 2s - loss: 0.1366 - acc: 0.7258 - 2s/epoch - 77ms/step\n",
      "Epoch 83/1500\n",
      "28/28 - 2s - loss: 0.1372 - acc: 0.7315 - 2s/epoch - 79ms/step\n",
      "Epoch 84/1500\n",
      "28/28 - 2s - loss: 0.1311 - acc: 0.7380 - 2s/epoch - 78ms/step\n",
      "Epoch 85/1500\n",
      "28/28 - 2s - loss: 0.1314 - acc: 0.7396 - 2s/epoch - 77ms/step\n",
      "Epoch 86/1500\n",
      "28/28 - 2s - loss: 0.1305 - acc: 0.7396 - 2s/epoch - 75ms/step\n",
      "Epoch 87/1500\n",
      "28/28 - 2s - loss: 0.1304 - acc: 0.7469 - 2s/epoch - 78ms/step\n",
      "Epoch 88/1500\n",
      "28/28 - 2s - loss: 0.1272 - acc: 0.7461 - 2s/epoch - 77ms/step\n",
      "Epoch 89/1500\n",
      "28/28 - 2s - loss: 0.1253 - acc: 0.7404 - 2s/epoch - 78ms/step\n",
      "Epoch 90/1500\n",
      "28/28 - 2s - loss: 0.1251 - acc: 0.7494 - 2s/epoch - 77ms/step\n",
      "Epoch 91/1500\n",
      "28/28 - 2s - loss: 0.1263 - acc: 0.7543 - 2s/epoch - 77ms/step\n",
      "Epoch 92/1500\n",
      "28/28 - 2s - loss: 0.1218 - acc: 0.7632 - 2s/epoch - 78ms/step\n",
      "Epoch 93/1500\n",
      "28/28 - 2s - loss: 0.1227 - acc: 0.7917 - 2s/epoch - 80ms/step\n",
      "Epoch 94/1500\n",
      "28/28 - 2s - loss: 0.1208 - acc: 0.7624 - 2s/epoch - 78ms/step\n",
      "Epoch 95/1500\n",
      "28/28 - 2s - loss: 0.1201 - acc: 0.7608 - 2s/epoch - 79ms/step\n",
      "Epoch 96/1500\n",
      "28/28 - 2s - loss: 0.1172 - acc: 0.7681 - 2s/epoch - 80ms/step\n",
      "Epoch 97/1500\n",
      "28/28 - 2s - loss: 0.1165 - acc: 0.7901 - 2s/epoch - 77ms/step\n",
      "Epoch 98/1500\n",
      "28/28 - 2s - loss: 0.1165 - acc: 0.7543 - 2s/epoch - 79ms/step\n",
      "Epoch 99/1500\n",
      "28/28 - 2s - loss: 0.1186 - acc: 0.7705 - 2s/epoch - 77ms/step\n",
      "Epoch 100/1500\n",
      "28/28 - 2s - loss: 0.1161 - acc: 0.7828 - 2s/epoch - 80ms/step\n",
      "Epoch 101/1500\n",
      "28/28 - 2s - loss: 0.1124 - acc: 0.7966 - 2s/epoch - 78ms/step\n",
      "Epoch 102/1500\n",
      "28/28 - 2s - loss: 0.1115 - acc: 0.7868 - 2s/epoch - 80ms/step\n",
      "Epoch 103/1500\n",
      "28/28 - 2s - loss: 0.1117 - acc: 0.7917 - 2s/epoch - 77ms/step\n",
      "Epoch 104/1500\n",
      "28/28 - 2s - loss: 0.1102 - acc: 0.7974 - 2s/epoch - 78ms/step\n",
      "Epoch 105/1500\n",
      "28/28 - 2s - loss: 0.1099 - acc: 0.7901 - 2s/epoch - 77ms/step\n",
      "Epoch 106/1500\n",
      "28/28 - 2s - loss: 0.1101 - acc: 0.7746 - 2s/epoch - 78ms/step\n",
      "Epoch 107/1500\n",
      "28/28 - 2s - loss: 0.1087 - acc: 0.8055 - 2s/epoch - 77ms/step\n",
      "Epoch 108/1500\n",
      "28/28 - 2s - loss: 0.1103 - acc: 0.8007 - 2s/epoch - 78ms/step\n",
      "Epoch 109/1500\n",
      "28/28 - 2s - loss: 0.1086 - acc: 0.7982 - 2s/epoch - 78ms/step\n",
      "Epoch 110/1500\n",
      "28/28 - 2s - loss: 0.1075 - acc: 0.8007 - 2s/epoch - 78ms/step\n",
      "Epoch 111/1500\n",
      "28/28 - 2s - loss: 0.1099 - acc: 0.7941 - 2s/epoch - 80ms/step\n",
      "Epoch 112/1500\n",
      "28/28 - 2s - loss: 0.1053 - acc: 0.8234 - 2s/epoch - 79ms/step\n",
      "Epoch 113/1500\n",
      "28/28 - 2s - loss: 0.1036 - acc: 0.8063 - 2s/epoch - 78ms/step\n",
      "Epoch 114/1500\n",
      "28/28 - 2s - loss: 0.1046 - acc: 0.8063 - 2s/epoch - 79ms/step\n",
      "Epoch 115/1500\n",
      "28/28 - 2s - loss: 0.1053 - acc: 0.7998 - 2s/epoch - 77ms/step\n",
      "Epoch 116/1500\n",
      "28/28 - 2s - loss: 0.1030 - acc: 0.8120 - 2s/epoch - 82ms/step\n",
      "Epoch 117/1500\n",
      "28/28 - 2s - loss: 0.1023 - acc: 0.8055 - 2s/epoch - 85ms/step\n",
      "Epoch 118/1500\n",
      "28/28 - 2s - loss: 0.1020 - acc: 0.8137 - 2s/epoch - 84ms/step\n",
      "Epoch 119/1500\n",
      "28/28 - 2s - loss: 0.1004 - acc: 0.8104 - 2s/epoch - 80ms/step\n",
      "Epoch 120/1500\n",
      "28/28 - 2s - loss: 0.1008 - acc: 0.8251 - 2s/epoch - 78ms/step\n",
      "Epoch 121/1500\n",
      "28/28 - 2s - loss: 0.1000 - acc: 0.8210 - 2s/epoch - 78ms/step\n",
      "Epoch 122/1500\n",
      "28/28 - 2s - loss: 0.0995 - acc: 0.7974 - 2s/epoch - 78ms/step\n",
      "Epoch 123/1500\n",
      "28/28 - 2s - loss: 0.1010 - acc: 0.8438 - 2s/epoch - 81ms/step\n",
      "Epoch 124/1500\n",
      "28/28 - 2s - loss: 0.0996 - acc: 0.8340 - 2s/epoch - 80ms/step\n",
      "Epoch 125/1500\n",
      "28/28 - 2s - loss: 0.1001 - acc: 0.8308 - 2s/epoch - 80ms/step\n",
      "Epoch 126/1500\n",
      "28/28 - 2s - loss: 0.1000 - acc: 0.8283 - 2s/epoch - 83ms/step\n",
      "Epoch 127/1500\n",
      "28/28 - 2s - loss: 0.0962 - acc: 0.8186 - 2s/epoch - 80ms/step\n",
      "Epoch 128/1500\n",
      "28/28 - 2s - loss: 0.0968 - acc: 0.8324 - 2s/epoch - 78ms/step\n",
      "Epoch 129/1500\n",
      "28/28 - 2s - loss: 0.0962 - acc: 0.8210 - 2s/epoch - 79ms/step\n",
      "Epoch 130/1500\n",
      "28/28 - 2s - loss: 0.0956 - acc: 0.8291 - 2s/epoch - 79ms/step\n",
      "Epoch 131/1500\n",
      "28/28 - 2s - loss: 0.0955 - acc: 0.7998 - 2s/epoch - 81ms/step\n",
      "Epoch 132/1500\n",
      "28/28 - 2s - loss: 0.0952 - acc: 0.8421 - 2s/epoch - 79ms/step\n",
      "Epoch 133/1500\n",
      "28/28 - 2s - loss: 0.0956 - acc: 0.8340 - 2s/epoch - 80ms/step\n",
      "Epoch 134/1500\n",
      "28/28 - 2s - loss: 0.0917 - acc: 0.8145 - 2s/epoch - 80ms/step\n",
      "Epoch 135/1500\n",
      "28/28 - 2s - loss: 0.0941 - acc: 0.8251 - 2s/epoch - 79ms/step\n",
      "Epoch 136/1500\n",
      "28/28 - 2s - loss: 0.0931 - acc: 0.8535 - 2s/epoch - 85ms/step\n",
      "Epoch 137/1500\n",
      "28/28 - 2s - loss: 0.0931 - acc: 0.8373 - 2s/epoch - 87ms/step\n",
      "Epoch 138/1500\n",
      "28/28 - 3s - loss: 0.0924 - acc: 0.8332 - 3s/epoch - 90ms/step\n",
      "Epoch 139/1500\n",
      "28/28 - 3s - loss: 0.0917 - acc: 0.8275 - 3s/epoch - 93ms/step\n",
      "Epoch 140/1500\n",
      "28/28 - 2s - loss: 0.0923 - acc: 0.8324 - 2s/epoch - 88ms/step\n",
      "Epoch 141/1500\n",
      "28/28 - 2s - loss: 0.0913 - acc: 0.8430 - 2s/epoch - 83ms/step\n",
      "Epoch 142/1500\n",
      "28/28 - 2s - loss: 0.0917 - acc: 0.8299 - 2s/epoch - 80ms/step\n",
      "Epoch 143/1500\n",
      "28/28 - 2s - loss: 0.0918 - acc: 0.8397 - 2s/epoch - 81ms/step\n",
      "Epoch 144/1500\n",
      "28/28 - 2s - loss: 0.0898 - acc: 0.8356 - 2s/epoch - 81ms/step\n",
      "Epoch 145/1500\n",
      "28/28 - 2s - loss: 0.0896 - acc: 0.8438 - 2s/epoch - 80ms/step\n",
      "Epoch 146/1500\n",
      "28/28 - 2s - loss: 0.0897 - acc: 0.8421 - 2s/epoch - 81ms/step\n",
      "Epoch 147/1500\n",
      "28/28 - 2s - loss: 0.0882 - acc: 0.8478 - 2s/epoch - 82ms/step\n",
      "Epoch 148/1500\n",
      "28/28 - 2s - loss: 0.0894 - acc: 0.8397 - 2s/epoch - 81ms/step\n",
      "Epoch 149/1500\n",
      "28/28 - 2s - loss: 0.0890 - acc: 0.8389 - 2s/epoch - 81ms/step\n",
      "Epoch 150/1500\n",
      "28/28 - 2s - loss: 0.0880 - acc: 0.8560 - 2s/epoch - 80ms/step\n",
      "Epoch 151/1500\n",
      "28/28 - 2s - loss: 0.0893 - acc: 0.8373 - 2s/epoch - 79ms/step\n",
      "Epoch 152/1500\n",
      "28/28 - 2s - loss: 0.0868 - acc: 0.8495 - 2s/epoch - 79ms/step\n",
      "Epoch 153/1500\n",
      "28/28 - 2s - loss: 0.0880 - acc: 0.8413 - 2s/epoch - 82ms/step\n",
      "Epoch 154/1500\n",
      "28/28 - 2s - loss: 0.0873 - acc: 0.8373 - 2s/epoch - 78ms/step\n",
      "Epoch 155/1500\n",
      "28/28 - 2s - loss: 0.0862 - acc: 0.8373 - 2s/epoch - 78ms/step\n",
      "Epoch 156/1500\n",
      "28/28 - 2s - loss: 0.0868 - acc: 0.8405 - 2s/epoch - 80ms/step\n",
      "Epoch 157/1500\n",
      "28/28 - 2s - loss: 0.0862 - acc: 0.8625 - 2s/epoch - 78ms/step\n",
      "Epoch 158/1500\n",
      "28/28 - 2s - loss: 0.0869 - acc: 0.8421 - 2s/epoch - 79ms/step\n",
      "Epoch 159/1500\n",
      "28/28 - 2s - loss: 0.0848 - acc: 0.8584 - 2s/epoch - 78ms/step\n",
      "Epoch 160/1500\n",
      "28/28 - 2s - loss: 0.0843 - acc: 0.8462 - 2s/epoch - 79ms/step\n",
      "Epoch 161/1500\n",
      "28/28 - 2s - loss: 0.0851 - acc: 0.8397 - 2s/epoch - 81ms/step\n",
      "Epoch 162/1500\n",
      "28/28 - 2s - loss: 0.0857 - acc: 0.8340 - 2s/epoch - 79ms/step\n",
      "Epoch 163/1500\n",
      "28/28 - 2s - loss: 0.0857 - acc: 0.8535 - 2s/epoch - 81ms/step\n",
      "Epoch 164/1500\n",
      "28/28 - 2s - loss: 0.0839 - acc: 0.8584 - 2s/epoch - 82ms/step\n",
      "Epoch 165/1500\n",
      "28/28 - 2s - loss: 0.0833 - acc: 0.8535 - 2s/epoch - 88ms/step\n",
      "Epoch 166/1500\n",
      "28/28 - 2s - loss: 0.0848 - acc: 0.8576 - 2s/epoch - 85ms/step\n",
      "Epoch 167/1500\n",
      "28/28 - 2s - loss: 0.0837 - acc: 0.8592 - 2s/epoch - 82ms/step\n",
      "Epoch 168/1500\n",
      "28/28 - 2s - loss: 0.0846 - acc: 0.8666 - 2s/epoch - 83ms/step\n",
      "Epoch 169/1500\n",
      "28/28 - 2s - loss: 0.0845 - acc: 0.8625 - 2s/epoch - 81ms/step\n",
      "Epoch 170/1500\n",
      "28/28 - 2s - loss: 0.0835 - acc: 0.8511 - 2s/epoch - 80ms/step\n",
      "Epoch 171/1500\n",
      "28/28 - 2s - loss: 0.0811 - acc: 0.8535 - 2s/epoch - 80ms/step\n",
      "Epoch 172/1500\n",
      "28/28 - 2s - loss: 0.0815 - acc: 0.8495 - 2s/epoch - 82ms/step\n",
      "Epoch 173/1500\n",
      "28/28 - 2s - loss: 0.0853 - acc: 0.8535 - 2s/epoch - 80ms/step\n",
      "Epoch 174/1500\n",
      "28/28 - 2s - loss: 0.0848 - acc: 0.8649 - 2s/epoch - 79ms/step\n",
      "Epoch 175/1500\n",
      "28/28 - 2s - loss: 0.0833 - acc: 0.8413 - 2s/epoch - 78ms/step\n",
      "Epoch 176/1500\n",
      "28/28 - 2s - loss: 0.0826 - acc: 0.8649 - 2s/epoch - 80ms/step\n",
      "Epoch 177/1500\n",
      "28/28 - 2s - loss: 0.0822 - acc: 0.8682 - 2s/epoch - 79ms/step\n",
      "Epoch 178/1500\n",
      "28/28 - 2s - loss: 0.0814 - acc: 0.8535 - 2s/epoch - 78ms/step\n",
      "Epoch 179/1500\n",
      "28/28 - 2s - loss: 0.0813 - acc: 0.8633 - 2s/epoch - 79ms/step\n",
      "Epoch 180/1500\n",
      "28/28 - 2s - loss: 0.0809 - acc: 0.8560 - 2s/epoch - 78ms/step\n",
      "Epoch 181/1500\n",
      "28/28 - 2s - loss: 0.0807 - acc: 0.8633 - 2s/epoch - 80ms/step\n",
      "Epoch 182/1500\n",
      "28/28 - 2s - loss: 0.0808 - acc: 0.8649 - 2s/epoch - 80ms/step\n",
      "Epoch 183/1500\n",
      "28/28 - 2s - loss: 0.0815 - acc: 0.8739 - 2s/epoch - 79ms/step\n",
      "Epoch 184/1500\n",
      "28/28 - 2s - loss: 0.0792 - acc: 0.8584 - 2s/epoch - 80ms/step\n",
      "Epoch 185/1500\n",
      "28/28 - 2s - loss: 0.0784 - acc: 0.8535 - 2s/epoch - 81ms/step\n",
      "Epoch 186/1500\n",
      "28/28 - 2s - loss: 0.0777 - acc: 0.8560 - 2s/epoch - 81ms/step\n",
      "Epoch 187/1500\n",
      "28/28 - 2s - loss: 0.0788 - acc: 0.8641 - 2s/epoch - 81ms/step\n",
      "Epoch 188/1500\n",
      "28/28 - 2s - loss: 0.0786 - acc: 0.8560 - 2s/epoch - 80ms/step\n",
      "Epoch 189/1500\n",
      "28/28 - 2s - loss: 0.0779 - acc: 0.8706 - 2s/epoch - 80ms/step\n",
      "Epoch 190/1500\n",
      "28/28 - 2s - loss: 0.0781 - acc: 0.8682 - 2s/epoch - 78ms/step\n",
      "Epoch 191/1500\n",
      "28/28 - 2s - loss: 0.0765 - acc: 0.8763 - 2s/epoch - 79ms/step\n",
      "Epoch 192/1500\n",
      "28/28 - 2s - loss: 0.0784 - acc: 0.8552 - 2s/epoch - 77ms/step\n",
      "Epoch 193/1500\n",
      "28/28 - 2s - loss: 0.0767 - acc: 0.8527 - 2s/epoch - 79ms/step\n",
      "Epoch 194/1500\n",
      "28/28 - 2s - loss: 0.0770 - acc: 0.8828 - 2s/epoch - 83ms/step\n",
      "Epoch 195/1500\n",
      "28/28 - 2s - loss: 0.0776 - acc: 0.8666 - 2s/epoch - 79ms/step\n",
      "Epoch 196/1500\n",
      "28/28 - 2s - loss: 0.0763 - acc: 0.8763 - 2s/epoch - 80ms/step\n",
      "Epoch 197/1500\n",
      "28/28 - 2s - loss: 0.0767 - acc: 0.8853 - 2s/epoch - 80ms/step\n",
      "Epoch 198/1500\n",
      "28/28 - 2s - loss: 0.0758 - acc: 0.8552 - 2s/epoch - 82ms/step\n",
      "Epoch 199/1500\n",
      "28/28 - 2s - loss: 0.0767 - acc: 0.8739 - 2s/epoch - 79ms/step\n",
      "Epoch 200/1500\n",
      "28/28 - 2s - loss: 0.0761 - acc: 0.8666 - 2s/epoch - 80ms/step\n",
      "Epoch 201/1500\n",
      "28/28 - 2s - loss: 0.0762 - acc: 0.8796 - 2s/epoch - 80ms/step\n",
      "Epoch 202/1500\n",
      "28/28 - 2s - loss: 0.0740 - acc: 0.8682 - 2s/epoch - 81ms/step\n",
      "Epoch 203/1500\n",
      "28/28 - 2s - loss: 0.0752 - acc: 0.8747 - 2s/epoch - 80ms/step\n",
      "Epoch 204/1500\n",
      "28/28 - 2s - loss: 0.0746 - acc: 0.8666 - 2s/epoch - 80ms/step\n",
      "Epoch 205/1500\n",
      "28/28 - 2s - loss: 0.0748 - acc: 0.8845 - 2s/epoch - 80ms/step\n",
      "Epoch 206/1500\n",
      "28/28 - 2s - loss: 0.0752 - acc: 0.8690 - 2s/epoch - 80ms/step\n",
      "Epoch 207/1500\n",
      "28/28 - 2s - loss: 0.0735 - acc: 0.8657 - 2s/epoch - 81ms/step\n",
      "Epoch 208/1500\n",
      "28/28 - 2s - loss: 0.0751 - acc: 0.8820 - 2s/epoch - 78ms/step\n",
      "Epoch 209/1500\n",
      "28/28 - 2s - loss: 0.0758 - acc: 0.8544 - 2s/epoch - 78ms/step\n",
      "Epoch 210/1500\n",
      "28/28 - 2s - loss: 0.0740 - acc: 0.8657 - 2s/epoch - 81ms/step\n",
      "Epoch 211/1500\n",
      "28/28 - 2s - loss: 0.0746 - acc: 0.8657 - 2s/epoch - 78ms/step\n",
      "Epoch 212/1500\n",
      "28/28 - 2s - loss: 0.0748 - acc: 0.8845 - 2s/epoch - 78ms/step\n",
      "Epoch 213/1500\n",
      "28/28 - 2s - loss: 0.0752 - acc: 0.8600 - 2s/epoch - 77ms/step\n",
      "Epoch 214/1500\n",
      "28/28 - 2s - loss: 0.0742 - acc: 0.8698 - 2s/epoch - 81ms/step\n",
      "Epoch 215/1500\n",
      "28/28 - 2s - loss: 0.0728 - acc: 0.8666 - 2s/epoch - 78ms/step\n",
      "Epoch 216/1500\n",
      "28/28 - 2s - loss: 0.0714 - acc: 0.8755 - 2s/epoch - 78ms/step\n",
      "Epoch 217/1500\n",
      "28/28 - 2s - loss: 0.0721 - acc: 0.8861 - 2s/epoch - 80ms/step\n",
      "Epoch 218/1500\n",
      "28/28 - 2s - loss: 0.0736 - acc: 0.8592 - 2s/epoch - 80ms/step\n",
      "Epoch 219/1500\n",
      "28/28 - 2s - loss: 0.0719 - acc: 0.8779 - 2s/epoch - 79ms/step\n",
      "Epoch 220/1500\n",
      "28/28 - 2s - loss: 0.0738 - acc: 0.8723 - 2s/epoch - 78ms/step\n",
      "Epoch 221/1500\n",
      "28/28 - 2s - loss: 0.0729 - acc: 0.8812 - 2s/epoch - 81ms/step\n",
      "Epoch 222/1500\n",
      "28/28 - 2s - loss: 0.0719 - acc: 0.8747 - 2s/epoch - 79ms/step\n",
      "Epoch 223/1500\n",
      "28/28 - 2s - loss: 0.0718 - acc: 0.8763 - 2s/epoch - 81ms/step\n",
      "Epoch 224/1500\n",
      "28/28 - 2s - loss: 0.0716 - acc: 0.8812 - 2s/epoch - 79ms/step\n",
      "Epoch 225/1500\n",
      "28/28 - 2s - loss: 0.0704 - acc: 0.8796 - 2s/epoch - 79ms/step\n",
      "Epoch 226/1500\n",
      "28/28 - 2s - loss: 0.0710 - acc: 0.8690 - 2s/epoch - 79ms/step\n",
      "Epoch 227/1500\n",
      "28/28 - 2s - loss: 0.0715 - acc: 0.8682 - 2s/epoch - 80ms/step\n",
      "Epoch 228/1500\n",
      "28/28 - 2s - loss: 0.0707 - acc: 0.8731 - 2s/epoch - 80ms/step\n",
      "Epoch 229/1500\n",
      "28/28 - 2s - loss: 0.0710 - acc: 0.8666 - 2s/epoch - 81ms/step\n",
      "Epoch 230/1500\n",
      "28/28 - 2s - loss: 0.0712 - acc: 0.8747 - 2s/epoch - 78ms/step\n",
      "Epoch 231/1500\n",
      "28/28 - 2s - loss: 0.0700 - acc: 0.8633 - 2s/epoch - 80ms/step\n",
      "Epoch 232/1500\n",
      "28/28 - 2s - loss: 0.0703 - acc: 0.8763 - 2s/epoch - 78ms/step\n",
      "Epoch 233/1500\n",
      "28/28 - 2s - loss: 0.0699 - acc: 0.8747 - 2s/epoch - 78ms/step\n",
      "Epoch 234/1500\n",
      "28/28 - 2s - loss: 0.0702 - acc: 0.8763 - 2s/epoch - 80ms/step\n",
      "Epoch 235/1500\n",
      "28/28 - 2s - loss: 0.0702 - acc: 0.8812 - 2s/epoch - 80ms/step\n",
      "Epoch 236/1500\n",
      "28/28 - 2s - loss: 0.0702 - acc: 0.8853 - 2s/epoch - 80ms/step\n",
      "Epoch 237/1500\n",
      "28/28 - 2s - loss: 0.0711 - acc: 0.8796 - 2s/epoch - 79ms/step\n",
      "Epoch 238/1500\n",
      "28/28 - 2s - loss: 0.0697 - acc: 0.8893 - 2s/epoch - 79ms/step\n",
      "Epoch 239/1500\n",
      "28/28 - 2s - loss: 0.0691 - acc: 0.8731 - 2s/epoch - 79ms/step\n",
      "Epoch 240/1500\n",
      "28/28 - 2s - loss: 0.0697 - acc: 0.8836 - 2s/epoch - 81ms/step\n",
      "Epoch 241/1500\n",
      "28/28 - 2s - loss: 0.0691 - acc: 0.8910 - 2s/epoch - 78ms/step\n",
      "Epoch 242/1500\n",
      "28/28 - 2s - loss: 0.0687 - acc: 0.8731 - 2s/epoch - 79ms/step\n",
      "Epoch 243/1500\n",
      "28/28 - 2s - loss: 0.0692 - acc: 0.8959 - 2s/epoch - 79ms/step\n",
      "Epoch 244/1500\n",
      "28/28 - 2s - loss: 0.0681 - acc: 0.8861 - 2s/epoch - 79ms/step\n",
      "Epoch 245/1500\n",
      "28/28 - 2s - loss: 0.0691 - acc: 0.8877 - 2s/epoch - 79ms/step\n",
      "Epoch 246/1500\n",
      "28/28 - 2s - loss: 0.0686 - acc: 0.8853 - 2s/epoch - 79ms/step\n",
      "Epoch 247/1500\n",
      "28/28 - 2s - loss: 0.0689 - acc: 0.8828 - 2s/epoch - 80ms/step\n",
      "Epoch 248/1500\n",
      "28/28 - 2s - loss: 0.0685 - acc: 0.8934 - 2s/epoch - 80ms/step\n",
      "Epoch 249/1500\n",
      "28/28 - 2s - loss: 0.0671 - acc: 0.8771 - 2s/epoch - 79ms/step\n",
      "Epoch 250/1500\n",
      "28/28 - 2s - loss: 0.0677 - acc: 0.8828 - 2s/epoch - 80ms/step\n",
      "Epoch 251/1500\n",
      "28/28 - 2s - loss: 0.0676 - acc: 0.8788 - 2s/epoch - 79ms/step\n",
      "Epoch 252/1500\n",
      "28/28 - 2s - loss: 0.0670 - acc: 0.8926 - 2s/epoch - 79ms/step\n",
      "Epoch 253/1500\n",
      "28/28 - 2s - loss: 0.0677 - acc: 0.8853 - 2s/epoch - 80ms/step\n",
      "Epoch 254/1500\n",
      "28/28 - 2s - loss: 0.0676 - acc: 0.8877 - 2s/epoch - 77ms/step\n",
      "Epoch 255/1500\n",
      "28/28 - 2s - loss: 0.0671 - acc: 0.8836 - 2s/epoch - 80ms/step\n",
      "Epoch 256/1500\n",
      "28/28 - 2s - loss: 0.0679 - acc: 0.8828 - 2s/epoch - 78ms/step\n",
      "Epoch 257/1500\n",
      "28/28 - 2s - loss: 0.0673 - acc: 0.8763 - 2s/epoch - 80ms/step\n",
      "Epoch 258/1500\n",
      "28/28 - 2s - loss: 0.0672 - acc: 0.8861 - 2s/epoch - 78ms/step\n",
      "Epoch 259/1500\n",
      "28/28 - 2s - loss: 0.0670 - acc: 0.8739 - 2s/epoch - 78ms/step\n",
      "Epoch 260/1500\n",
      "28/28 - 2s - loss: 0.0669 - acc: 0.8649 - 2s/epoch - 79ms/step\n",
      "Epoch 261/1500\n",
      "28/28 - 2s - loss: 0.0650 - acc: 0.8869 - 2s/epoch - 79ms/step\n",
      "Epoch 262/1500\n",
      "28/28 - 2s - loss: 0.0664 - acc: 0.8755 - 2s/epoch - 79ms/step\n",
      "Epoch 263/1500\n",
      "28/28 - 2s - loss: 0.0665 - acc: 0.8926 - 2s/epoch - 77ms/step\n",
      "Epoch 264/1500\n",
      "28/28 - 2s - loss: 0.0648 - acc: 0.8845 - 2s/epoch - 78ms/step\n",
      "Epoch 265/1500\n",
      "28/28 - 2s - loss: 0.0651 - acc: 0.8877 - 2s/epoch - 79ms/step\n",
      "Epoch 266/1500\n",
      "28/28 - 2s - loss: 0.0652 - acc: 0.8706 - 2s/epoch - 79ms/step\n",
      "Epoch 267/1500\n",
      "28/28 - 2s - loss: 0.0653 - acc: 0.8910 - 2s/epoch - 77ms/step\n",
      "Epoch 268/1500\n",
      "28/28 - 2s - loss: 0.0648 - acc: 0.8893 - 2s/epoch - 77ms/step\n",
      "Epoch 269/1500\n",
      "28/28 - 2s - loss: 0.0647 - acc: 0.9048 - 2s/epoch - 79ms/step\n",
      "Epoch 270/1500\n",
      "28/28 - 2s - loss: 0.0655 - acc: 0.8763 - 2s/epoch - 81ms/step\n",
      "Epoch 271/1500\n",
      "28/28 - 2s - loss: 0.0656 - acc: 0.8828 - 2s/epoch - 80ms/step\n",
      "Epoch 272/1500\n",
      "28/28 - 2s - loss: 0.0646 - acc: 0.8820 - 2s/epoch - 79ms/step\n",
      "Epoch 273/1500\n",
      "28/28 - 2s - loss: 0.0639 - acc: 0.8820 - 2s/epoch - 79ms/step\n",
      "Epoch 274/1500\n",
      "28/28 - 2s - loss: 0.0669 - acc: 0.8845 - 2s/epoch - 81ms/step\n",
      "Epoch 275/1500\n",
      "28/28 - 2s - loss: 0.0658 - acc: 0.8853 - 2s/epoch - 79ms/step\n",
      "Epoch 276/1500\n",
      "28/28 - 2s - loss: 0.0650 - acc: 0.8934 - 2s/epoch - 79ms/step\n",
      "Epoch 277/1500\n",
      "28/28 - 2s - loss: 0.0647 - acc: 0.8796 - 2s/epoch - 82ms/step\n",
      "Epoch 278/1500\n",
      "28/28 - 2s - loss: 0.0638 - acc: 0.8828 - 2s/epoch - 79ms/step\n",
      "Epoch 279/1500\n",
      "28/28 - 2s - loss: 0.0639 - acc: 0.8967 - 2s/epoch - 80ms/step\n",
      "Epoch 280/1500\n",
      "28/28 - 2s - loss: 0.0640 - acc: 0.8828 - 2s/epoch - 78ms/step\n",
      "Epoch 281/1500\n",
      "28/28 - 2s - loss: 0.0629 - acc: 0.8861 - 2s/epoch - 79ms/step\n",
      "Epoch 282/1500\n",
      "28/28 - 2s - loss: 0.0634 - acc: 0.8991 - 2s/epoch - 79ms/step\n",
      "Epoch 283/1500\n",
      "28/28 - 2s - loss: 0.0633 - acc: 0.8926 - 2s/epoch - 81ms/step\n",
      "Epoch 284/1500\n",
      "28/28 - 2s - loss: 0.0631 - acc: 0.8788 - 2s/epoch - 79ms/step\n",
      "Epoch 285/1500\n",
      "28/28 - 2s - loss: 0.0635 - acc: 0.8991 - 2s/epoch - 78ms/step\n",
      "Epoch 286/1500\n",
      "28/28 - 2s - loss: 0.0625 - acc: 0.8836 - 2s/epoch - 80ms/step\n",
      "Epoch 287/1500\n",
      "28/28 - 2s - loss: 0.0628 - acc: 0.8950 - 2s/epoch - 80ms/step\n",
      "Epoch 288/1500\n",
      "28/28 - 2s - loss: 0.0618 - acc: 0.8845 - 2s/epoch - 79ms/step\n",
      "Epoch 289/1500\n",
      "28/28 - 2s - loss: 0.0639 - acc: 0.8861 - 2s/epoch - 78ms/step\n",
      "Epoch 290/1500\n",
      "28/28 - 2s - loss: 0.0629 - acc: 0.8820 - 2s/epoch - 77ms/step\n",
      "Epoch 291/1500\n",
      "28/28 - 2s - loss: 0.0632 - acc: 0.8853 - 2s/epoch - 79ms/step\n",
      "Epoch 292/1500\n",
      "28/28 - 2s - loss: 0.0623 - acc: 0.8918 - 2s/epoch - 79ms/step\n",
      "Epoch 293/1500\n",
      "28/28 - 2s - loss: 0.0628 - acc: 0.8902 - 2s/epoch - 78ms/step\n",
      "Epoch 294/1500\n",
      "28/28 - 2s - loss: 0.0629 - acc: 0.8893 - 2s/epoch - 78ms/step\n",
      "Epoch 295/1500\n",
      "28/28 - 2s - loss: 0.0615 - acc: 0.8828 - 2s/epoch - 79ms/step\n",
      "Epoch 296/1500\n",
      "28/28 - 2s - loss: 0.0620 - acc: 0.8926 - 2s/epoch - 82ms/step\n",
      "Epoch 297/1500\n",
      "28/28 - 2s - loss: 0.0623 - acc: 0.8853 - 2s/epoch - 78ms/step\n",
      "Epoch 298/1500\n",
      "28/28 - 2s - loss: 0.0615 - acc: 0.8853 - 2s/epoch - 79ms/step\n",
      "Epoch 299/1500\n",
      "28/28 - 2s - loss: 0.0619 - acc: 0.8836 - 2s/epoch - 78ms/step\n",
      "Epoch 300/1500\n",
      "28/28 - 2s - loss: 0.0623 - acc: 0.8959 - 2s/epoch - 79ms/step\n",
      "Epoch 301/1500\n",
      "28/28 - 2s - loss: 0.0626 - acc: 0.8926 - 2s/epoch - 80ms/step\n",
      "Epoch 302/1500\n",
      "28/28 - 2s - loss: 0.0627 - acc: 0.9040 - 2s/epoch - 78ms/step\n",
      "Epoch 303/1500\n",
      "28/28 - 2s - loss: 0.0622 - acc: 0.8836 - 2s/epoch - 79ms/step\n",
      "Epoch 304/1500\n",
      "28/28 - 2s - loss: 0.0621 - acc: 0.8902 - 2s/epoch - 78ms/step\n",
      "Epoch 305/1500\n",
      "28/28 - 2s - loss: 0.0616 - acc: 0.8902 - 2s/epoch - 80ms/step\n",
      "Epoch 306/1500\n",
      "28/28 - 2s - loss: 0.0607 - acc: 0.8885 - 2s/epoch - 81ms/step\n",
      "Epoch 307/1500\n",
      "28/28 - 2s - loss: 0.0606 - acc: 0.8877 - 2s/epoch - 78ms/step\n",
      "Epoch 308/1500\n",
      "28/28 - 2s - loss: 0.0605 - acc: 0.8869 - 2s/epoch - 77ms/step\n",
      "Epoch 309/1500\n",
      "28/28 - 2s - loss: 0.0610 - acc: 0.8967 - 2s/epoch - 77ms/step\n",
      "Epoch 310/1500\n",
      "28/28 - 2s - loss: 0.0606 - acc: 0.8950 - 2s/epoch - 79ms/step\n",
      "Epoch 311/1500\n",
      "28/28 - 2s - loss: 0.0601 - acc: 0.8902 - 2s/epoch - 78ms/step\n",
      "Epoch 312/1500\n",
      "28/28 - 2s - loss: 0.0617 - acc: 0.8934 - 2s/epoch - 78ms/step\n",
      "Epoch 313/1500\n",
      "28/28 - 2s - loss: 0.0601 - acc: 0.8926 - 2s/epoch - 79ms/step\n",
      "Epoch 314/1500\n",
      "28/28 - 2s - loss: 0.0590 - acc: 0.9007 - 2s/epoch - 79ms/step\n",
      "Epoch 315/1500\n",
      "28/28 - 2s - loss: 0.0599 - acc: 0.8918 - 2s/epoch - 78ms/step\n",
      "Epoch 316/1500\n",
      "28/28 - 2s - loss: 0.0601 - acc: 0.8950 - 2s/epoch - 78ms/step\n",
      "Epoch 317/1500\n",
      "28/28 - 2s - loss: 0.0593 - acc: 0.8926 - 2s/epoch - 78ms/step\n",
      "Epoch 318/1500\n",
      "28/28 - 2s - loss: 0.0591 - acc: 0.9105 - 2s/epoch - 78ms/step\n",
      "Epoch 319/1500\n",
      "28/28 - 2s - loss: 0.0591 - acc: 0.8926 - 2s/epoch - 78ms/step\n",
      "Epoch 320/1500\n",
      "28/28 - 2s - loss: 0.0593 - acc: 0.9032 - 2s/epoch - 79ms/step\n",
      "Epoch 321/1500\n",
      "28/28 - 2s - loss: 0.0593 - acc: 0.8885 - 2s/epoch - 78ms/step\n",
      "Epoch 322/1500\n",
      "28/28 - 2s - loss: 0.0588 - acc: 0.8885 - 2s/epoch - 78ms/step\n",
      "Epoch 323/1500\n",
      "28/28 - 2s - loss: 0.0594 - acc: 0.8788 - 2s/epoch - 80ms/step\n",
      "Epoch 324/1500\n",
      "28/28 - 2s - loss: 0.0592 - acc: 0.9056 - 2s/epoch - 79ms/step\n",
      "Epoch 325/1500\n",
      "28/28 - 2s - loss: 0.0592 - acc: 0.8934 - 2s/epoch - 78ms/step\n",
      "Epoch 326/1500\n",
      "28/28 - 2s - loss: 0.0584 - acc: 0.9064 - 2s/epoch - 78ms/step\n",
      "Epoch 327/1500\n",
      "28/28 - 2s - loss: 0.0586 - acc: 0.8934 - 2s/epoch - 79ms/step\n",
      "Epoch 328/1500\n",
      "28/28 - 2s - loss: 0.0587 - acc: 0.8950 - 2s/epoch - 78ms/step\n",
      "Epoch 329/1500\n",
      "28/28 - 2s - loss: 0.0583 - acc: 0.8999 - 2s/epoch - 78ms/step\n",
      "Epoch 330/1500\n",
      "28/28 - 2s - loss: 0.0588 - acc: 0.8967 - 2s/epoch - 78ms/step\n",
      "Epoch 331/1500\n",
      "28/28 - 2s - loss: 0.0591 - acc: 0.8812 - 2s/epoch - 78ms/step\n",
      "Epoch 332/1500\n",
      "28/28 - 2s - loss: 0.0581 - acc: 0.8902 - 2s/epoch - 80ms/step\n",
      "Epoch 333/1500\n",
      "28/28 - 2s - loss: 0.0589 - acc: 0.8902 - 2s/epoch - 78ms/step\n",
      "Epoch 334/1500\n",
      "28/28 - 2s - loss: 0.0583 - acc: 0.8942 - 2s/epoch - 77ms/step\n",
      "Epoch 335/1500\n",
      "28/28 - 2s - loss: 0.0589 - acc: 0.8950 - 2s/epoch - 78ms/step\n",
      "Epoch 336/1500\n",
      "28/28 - 2s - loss: 0.0581 - acc: 0.8942 - 2s/epoch - 81ms/step\n",
      "Epoch 337/1500\n",
      "28/28 - 2s - loss: 0.0578 - acc: 0.9048 - 2s/epoch - 83ms/step\n",
      "Epoch 338/1500\n",
      "28/28 - 3s - loss: 0.0570 - acc: 0.8983 - 3s/epoch - 92ms/step\n",
      "Epoch 339/1500\n",
      "28/28 - 2s - loss: 0.0572 - acc: 0.8918 - 2s/epoch - 84ms/step\n",
      "Epoch 340/1500\n",
      "28/28 - 2s - loss: 0.0573 - acc: 0.8991 - 2s/epoch - 83ms/step\n",
      "Epoch 341/1500\n",
      "28/28 - 2s - loss: 0.0582 - acc: 0.9015 - 2s/epoch - 85ms/step\n",
      "Epoch 342/1500\n",
      "28/28 - 2s - loss: 0.0576 - acc: 0.8959 - 2s/epoch - 89ms/step\n",
      "Epoch 343/1500\n",
      "28/28 - 2s - loss: 0.0576 - acc: 0.9024 - 2s/epoch - 89ms/step\n",
      "Epoch 344/1500\n",
      "28/28 - 2s - loss: 0.0576 - acc: 0.8967 - 2s/epoch - 79ms/step\n",
      "Epoch 345/1500\n",
      "28/28 - 2s - loss: 0.0562 - acc: 0.9138 - 2s/epoch - 73ms/step\n",
      "Epoch 346/1500\n",
      "28/28 - 2s - loss: 0.0577 - acc: 0.8902 - 2s/epoch - 75ms/step\n",
      "Epoch 347/1500\n",
      "28/28 - 2s - loss: 0.0572 - acc: 0.8991 - 2s/epoch - 73ms/step\n",
      "Epoch 348/1500\n",
      "28/28 - 2s - loss: 0.0571 - acc: 0.8893 - 2s/epoch - 73ms/step\n",
      "Epoch 349/1500\n",
      "28/28 - 2s - loss: 0.0574 - acc: 0.8918 - 2s/epoch - 72ms/step\n",
      "Epoch 350/1500\n",
      "28/28 - 2s - loss: 0.0565 - acc: 0.8975 - 2s/epoch - 72ms/step\n",
      "Epoch 351/1500\n",
      "28/28 - 2s - loss: 0.0556 - acc: 0.9129 - 2s/epoch - 74ms/step\n",
      "Epoch 352/1500\n",
      "28/28 - 2s - loss: 0.0565 - acc: 0.9007 - 2s/epoch - 72ms/step\n",
      "Epoch 353/1500\n",
      "28/28 - 2s - loss: 0.0560 - acc: 0.9048 - 2s/epoch - 72ms/step\n",
      "Epoch 354/1500\n",
      "28/28 - 2s - loss: 0.0569 - acc: 0.8918 - 2s/epoch - 72ms/step\n",
      "Epoch 355/1500\n",
      "28/28 - 2s - loss: 0.0569 - acc: 0.8999 - 2s/epoch - 72ms/step\n",
      "Epoch 356/1500\n",
      "28/28 - 2s - loss: 0.0564 - acc: 0.9138 - 2s/epoch - 79ms/step\n",
      "Epoch 357/1500\n",
      "28/28 - 2s - loss: 0.0559 - acc: 0.9056 - 2s/epoch - 76ms/step\n",
      "Epoch 358/1500\n",
      "28/28 - 2s - loss: 0.0568 - acc: 0.8942 - 2s/epoch - 74ms/step\n",
      "Epoch 359/1500\n",
      "28/28 - 2s - loss: 0.0563 - acc: 0.9056 - 2s/epoch - 78ms/step\n",
      "Epoch 360/1500\n",
      "28/28 - 2s - loss: 0.0568 - acc: 0.8950 - 2s/epoch - 74ms/step\n",
      "Epoch 361/1500\n",
      "28/28 - 2s - loss: 0.0567 - acc: 0.9113 - 2s/epoch - 82ms/step\n",
      "Epoch 362/1500\n",
      "28/28 - 2s - loss: 0.0572 - acc: 0.9268 - 2s/epoch - 78ms/step\n",
      "Epoch 363/1500\n",
      "28/28 - 2s - loss: 0.0566 - acc: 0.8836 - 2s/epoch - 84ms/step\n",
      "Epoch 364/1500\n",
      "28/28 - 2s - loss: 0.0556 - acc: 0.8959 - 2s/epoch - 82ms/step\n",
      "Epoch 365/1500\n",
      "28/28 - 2s - loss: 0.0561 - acc: 0.8918 - 2s/epoch - 78ms/step\n",
      "Epoch 366/1500\n",
      "28/28 - 2s - loss: 0.0551 - acc: 0.8983 - 2s/epoch - 79ms/step\n",
      "Epoch 367/1500\n",
      "28/28 - 2s - loss: 0.0561 - acc: 0.9040 - 2s/epoch - 75ms/step\n",
      "Epoch 368/1500\n",
      "28/28 - 2s - loss: 0.0551 - acc: 0.9015 - 2s/epoch - 75ms/step\n",
      "Epoch 369/1500\n",
      "28/28 - 2s - loss: 0.0552 - acc: 0.9048 - 2s/epoch - 75ms/step\n",
      "Epoch 370/1500\n",
      "28/28 - 2s - loss: 0.0551 - acc: 0.8983 - 2s/epoch - 73ms/step\n",
      "Epoch 371/1500\n",
      "28/28 - 2s - loss: 0.0561 - acc: 0.9032 - 2s/epoch - 74ms/step\n",
      "Epoch 372/1500\n",
      "28/28 - 2s - loss: 0.0548 - acc: 0.9089 - 2s/epoch - 75ms/step\n",
      "Epoch 373/1500\n",
      "28/28 - 2s - loss: 0.0553 - acc: 0.8983 - 2s/epoch - 73ms/step\n",
      "Epoch 374/1500\n",
      "28/28 - 2s - loss: 0.0548 - acc: 0.9032 - 2s/epoch - 73ms/step\n",
      "Epoch 375/1500\n",
      "28/28 - 2s - loss: 0.0542 - acc: 0.8991 - 2s/epoch - 75ms/step\n",
      "Epoch 376/1500\n",
      "28/28 - 2s - loss: 0.0538 - acc: 0.8991 - 2s/epoch - 77ms/step\n",
      "Epoch 377/1500\n",
      "28/28 - 2s - loss: 0.0538 - acc: 0.8967 - 2s/epoch - 75ms/step\n",
      "Epoch 378/1500\n",
      "28/28 - 2s - loss: 0.0546 - acc: 0.9129 - 2s/epoch - 73ms/step\n",
      "Epoch 379/1500\n",
      "28/28 - 2s - loss: 0.0539 - acc: 0.9040 - 2s/epoch - 72ms/step\n",
      "Epoch 380/1500\n",
      "28/28 - 2s - loss: 0.0543 - acc: 0.8999 - 2s/epoch - 75ms/step\n",
      "Epoch 381/1500\n",
      "28/28 - 2s - loss: 0.0541 - acc: 0.9032 - 2s/epoch - 76ms/step\n",
      "Epoch 382/1500\n",
      "28/28 - 2s - loss: 0.0538 - acc: 0.8959 - 2s/epoch - 76ms/step\n",
      "Epoch 383/1500\n",
      "28/28 - 2s - loss: 0.0534 - acc: 0.8926 - 2s/epoch - 75ms/step\n",
      "Epoch 384/1500\n",
      "28/28 - 2s - loss: 0.0543 - acc: 0.9040 - 2s/epoch - 74ms/step\n",
      "Epoch 385/1500\n",
      "28/28 - 2s - loss: 0.0548 - acc: 0.9007 - 2s/epoch - 75ms/step\n",
      "Epoch 386/1500\n",
      "28/28 - 2s - loss: 0.0543 - acc: 0.9032 - 2s/epoch - 76ms/step\n",
      "Epoch 387/1500\n",
      "28/28 - 2s - loss: 0.0542 - acc: 0.8975 - 2s/epoch - 80ms/step\n",
      "Epoch 388/1500\n",
      "28/28 - 2s - loss: 0.0535 - acc: 0.9040 - 2s/epoch - 77ms/step\n",
      "Epoch 389/1500\n",
      "28/28 - 2s - loss: 0.0542 - acc: 0.8999 - 2s/epoch - 76ms/step\n",
      "Epoch 390/1500\n",
      "28/28 - 2s - loss: 0.0545 - acc: 0.9056 - 2s/epoch - 75ms/step\n",
      "Epoch 391/1500\n",
      "28/28 - 2s - loss: 0.0535 - acc: 0.8999 - 2s/epoch - 85ms/step\n",
      "Epoch 392/1500\n",
      "28/28 - 2s - loss: 0.0525 - acc: 0.9097 - 2s/epoch - 79ms/step\n",
      "Epoch 393/1500\n",
      "28/28 - 2s - loss: 0.0547 - acc: 0.9048 - 2s/epoch - 80ms/step\n",
      "Epoch 394/1500\n",
      "28/28 - 2s - loss: 0.0539 - acc: 0.9064 - 2s/epoch - 79ms/step\n",
      "Epoch 395/1500\n",
      "28/28 - 2s - loss: 0.0523 - acc: 0.9056 - 2s/epoch - 77ms/step\n",
      "Epoch 396/1500\n",
      "28/28 - 2s - loss: 0.0529 - acc: 0.9064 - 2s/epoch - 74ms/step\n",
      "Epoch 397/1500\n",
      "28/28 - 2s - loss: 0.0537 - acc: 0.9129 - 2s/epoch - 75ms/step\n",
      "Epoch 398/1500\n",
      "28/28 - 2s - loss: 0.0525 - acc: 0.9113 - 2s/epoch - 75ms/step\n",
      "Epoch 399/1500\n",
      "28/28 - 2s - loss: 0.0534 - acc: 0.9211 - 2s/epoch - 76ms/step\n",
      "Epoch 400/1500\n",
      "28/28 - 2s - loss: 0.0530 - acc: 0.9072 - 2s/epoch - 83ms/step\n",
      "Epoch 401/1500\n",
      "28/28 - 2s - loss: 0.0521 - acc: 0.8999 - 2s/epoch - 81ms/step\n",
      "Epoch 402/1500\n",
      "28/28 - 2s - loss: 0.0524 - acc: 0.9048 - 2s/epoch - 81ms/step\n",
      "Epoch 403/1500\n",
      "28/28 - 2s - loss: 0.0528 - acc: 0.9162 - 2s/epoch - 76ms/step\n",
      "Epoch 404/1500\n",
      "28/28 - 2s - loss: 0.0535 - acc: 0.9097 - 2s/epoch - 75ms/step\n",
      "Epoch 405/1500\n",
      "28/28 - 2s - loss: 0.0532 - acc: 0.9072 - 2s/epoch - 81ms/step\n",
      "Epoch 406/1500\n",
      "28/28 - 2s - loss: 0.0534 - acc: 0.9024 - 2s/epoch - 80ms/step\n",
      "Epoch 407/1500\n",
      "28/28 - 2s - loss: 0.0534 - acc: 0.9081 - 2s/epoch - 76ms/step\n",
      "Epoch 408/1500\n",
      "28/28 - 2s - loss: 0.0524 - acc: 0.9032 - 2s/epoch - 81ms/step\n",
      "Epoch 409/1500\n",
      "28/28 - 2s - loss: 0.0523 - acc: 0.9056 - 2s/epoch - 77ms/step\n",
      "Epoch 410/1500\n",
      "28/28 - 2s - loss: 0.0523 - acc: 0.9040 - 2s/epoch - 75ms/step\n",
      "Epoch 411/1500\n",
      "28/28 - 2s - loss: 0.0523 - acc: 0.9007 - 2s/epoch - 79ms/step\n",
      "Epoch 412/1500\n",
      "28/28 - 2s - loss: 0.0531 - acc: 0.9024 - 2s/epoch - 75ms/step\n",
      "Epoch 413/1500\n",
      "28/28 - 2s - loss: 0.0520 - acc: 0.9007 - 2s/epoch - 82ms/step\n",
      "Epoch 414/1500\n",
      "28/28 - 2s - loss: 0.0517 - acc: 0.9015 - 2s/epoch - 81ms/step\n",
      "Epoch 415/1500\n",
      "28/28 - 2s - loss: 0.0517 - acc: 0.9129 - 2s/epoch - 75ms/step\n",
      "Epoch 416/1500\n",
      "28/28 - 2s - loss: 0.0517 - acc: 0.9097 - 2s/epoch - 75ms/step\n",
      "Epoch 417/1500\n",
      "28/28 - 2s - loss: 0.0513 - acc: 0.9219 - 2s/epoch - 79ms/step\n",
      "Epoch 418/1500\n",
      "28/28 - 2s - loss: 0.0514 - acc: 0.9024 - 2s/epoch - 80ms/step\n",
      "Epoch 419/1500\n",
      "28/28 - 2s - loss: 0.0518 - acc: 0.9251 - 2s/epoch - 82ms/step\n",
      "Epoch 420/1500\n",
      "28/28 - 2s - loss: 0.0517 - acc: 0.9194 - 2s/epoch - 82ms/step\n",
      "Epoch 421/1500\n",
      "28/28 - 2s - loss: 0.0522 - acc: 0.9032 - 2s/epoch - 81ms/step\n",
      "Epoch 422/1500\n",
      "28/28 - 2s - loss: 0.0512 - acc: 0.8999 - 2s/epoch - 78ms/step\n",
      "Epoch 423/1500\n",
      "28/28 - 2s - loss: 0.0514 - acc: 0.9097 - 2s/epoch - 81ms/step\n",
      "Epoch 424/1500\n",
      "28/28 - 2s - loss: 0.0519 - acc: 0.9227 - 2s/epoch - 78ms/step\n",
      "Epoch 425/1500\n",
      "28/28 - 2s - loss: 0.0501 - acc: 0.9032 - 2s/epoch - 78ms/step\n",
      "Epoch 426/1500\n",
      "28/28 - 2s - loss: 0.0513 - acc: 0.9032 - 2s/epoch - 77ms/step\n",
      "Epoch 427/1500\n",
      "28/28 - 2s - loss: 0.0512 - acc: 0.9154 - 2s/epoch - 79ms/step\n",
      "Epoch 428/1500\n",
      "28/28 - 2s - loss: 0.0512 - acc: 0.9154 - 2s/epoch - 77ms/step\n",
      "Epoch 429/1500\n",
      "28/28 - 2s - loss: 0.0519 - acc: 0.9024 - 2s/epoch - 76ms/step\n",
      "Epoch 430/1500\n",
      "28/28 - 2s - loss: 0.0518 - acc: 0.9162 - 2s/epoch - 81ms/step\n",
      "Epoch 431/1500\n",
      "28/28 - 2s - loss: 0.0510 - acc: 0.9032 - 2s/epoch - 79ms/step\n",
      "Epoch 432/1500\n",
      "28/28 - 2s - loss: 0.0505 - acc: 0.9113 - 2s/epoch - 74ms/step\n",
      "Epoch 433/1500\n",
      "28/28 - 2s - loss: 0.0501 - acc: 0.9089 - 2s/epoch - 80ms/step\n",
      "Epoch 434/1500\n",
      "28/28 - 2s - loss: 0.0508 - acc: 0.9146 - 2s/epoch - 78ms/step\n",
      "Epoch 435/1500\n",
      "28/28 - 2s - loss: 0.0502 - acc: 0.9154 - 2s/epoch - 76ms/step\n",
      "Epoch 436/1500\n",
      "28/28 - 2s - loss: 0.0506 - acc: 0.9089 - 2s/epoch - 75ms/step\n",
      "Epoch 437/1500\n",
      "28/28 - 2s - loss: 0.0504 - acc: 0.9219 - 2s/epoch - 79ms/step\n",
      "Epoch 438/1500\n",
      "28/28 - 2s - loss: 0.0505 - acc: 0.9186 - 2s/epoch - 74ms/step\n",
      "Epoch 439/1500\n",
      "28/28 - 2s - loss: 0.0505 - acc: 0.9211 - 2s/epoch - 73ms/step\n",
      "Epoch 440/1500\n",
      "28/28 - 2s - loss: 0.0510 - acc: 0.9064 - 2s/epoch - 79ms/step\n",
      "Epoch 441/1500\n",
      "28/28 - 2s - loss: 0.0505 - acc: 0.9138 - 2s/epoch - 80ms/step\n",
      "Epoch 442/1500\n",
      "28/28 - 2s - loss: 0.0510 - acc: 0.9113 - 2s/epoch - 77ms/step\n",
      "Epoch 443/1500\n",
      "28/28 - 2s - loss: 0.0510 - acc: 0.9097 - 2s/epoch - 76ms/step\n",
      "Epoch 444/1500\n",
      "28/28 - 2s - loss: 0.0499 - acc: 0.9170 - 2s/epoch - 74ms/step\n",
      "Epoch 445/1500\n",
      "28/28 - 2s - loss: 0.0497 - acc: 0.9048 - 2s/epoch - 80ms/step\n",
      "Epoch 446/1500\n",
      "28/28 - 2s - loss: 0.0501 - acc: 0.9186 - 2s/epoch - 77ms/step\n",
      "Epoch 447/1500\n",
      "28/28 - 2s - loss: 0.0503 - acc: 0.9186 - 2s/epoch - 82ms/step\n",
      "Epoch 448/1500\n",
      "28/28 - 2s - loss: 0.0505 - acc: 0.9146 - 2s/epoch - 76ms/step\n",
      "Epoch 449/1500\n",
      "28/28 - 2s - loss: 0.0496 - acc: 0.8983 - 2s/epoch - 80ms/step\n",
      "Epoch 450/1500\n",
      "28/28 - 2s - loss: 0.0498 - acc: 0.9040 - 2s/epoch - 75ms/step\n",
      "Epoch 451/1500\n",
      "28/28 - 2s - loss: 0.0500 - acc: 0.9170 - 2s/epoch - 80ms/step\n",
      "Epoch 452/1500\n",
      "28/28 - 2s - loss: 0.0495 - acc: 0.9162 - 2s/epoch - 77ms/step\n",
      "Epoch 453/1500\n",
      "28/28 - 2s - loss: 0.0505 - acc: 0.9138 - 2s/epoch - 76ms/step\n",
      "Epoch 454/1500\n",
      "28/28 - 2s - loss: 0.0496 - acc: 0.9105 - 2s/epoch - 76ms/step\n",
      "Epoch 455/1500\n",
      "28/28 - 2s - loss: 0.0496 - acc: 0.9162 - 2s/epoch - 76ms/step\n",
      "Epoch 456/1500\n",
      "28/28 - 2s - loss: 0.0495 - acc: 0.9138 - 2s/epoch - 76ms/step\n",
      "Epoch 457/1500\n",
      "28/28 - 2s - loss: 0.0492 - acc: 0.8991 - 2s/epoch - 74ms/step\n",
      "Epoch 458/1500\n",
      "28/28 - 2s - loss: 0.0482 - acc: 0.9170 - 2s/epoch - 77ms/step\n",
      "Epoch 459/1500\n",
      "28/28 - 2s - loss: 0.0488 - acc: 0.9056 - 2s/epoch - 77ms/step\n",
      "Epoch 460/1500\n",
      "28/28 - 2s - loss: 0.0488 - acc: 0.9178 - 2s/epoch - 78ms/step\n",
      "Epoch 461/1500\n",
      "28/28 - 2s - loss: 0.0485 - acc: 0.9146 - 2s/epoch - 84ms/step\n",
      "Epoch 462/1500\n",
      "28/28 - 2s - loss: 0.0494 - acc: 0.9146 - 2s/epoch - 83ms/step\n",
      "Epoch 00462: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "history=model.fit(train, label, batch_size=BATCHSIZE, epochs=EPOCH, verbose=2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de perte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAlL0lEQVR4nO3deXTcZ33v8fd3du37ZsmLEtuxncVOojiBAAmlgJ0AhqSEbCShi296yT309MDFtAdaKJzSlnsJXJK4IU1ogZJCQ0ighhQM2TBZlMQm3i1vkWxrsWxtlkbSzDz3j5k4iiLbI3mk0cx8XufM8fyWmfnq8fHHj555fs/PnHOIiEjm86S7ABERSQ0FuohIllCgi4hkCQW6iEiWUKCLiGQJX7o+uLKy0i1YsCBdHy8ikpFeeumlo865qomOpS3QFyxYQHNzc7o+XkQkI5nZwVMd05CLiEiWUKCLiGQJBbqISJZI2xi6iGSf0dFR2traCIfD6S4l44VCIRoaGvD7/Um/RoEuIinT1tZGUVERCxYswMzSXU7Gcs7R3d1NW1sbjY2NSb8uqSEXM1tlZrvMrMXM1k1wvMTMfmpmW8xsm5l9YhK1i0iWCIfDVFRUKMzPkplRUVEx6d90zhjoZuYF7gFWA8uAm8xs2bjTPglsd84tB64G/o+ZBSZViYhkBYV5akylHZPpoa8EWpxz+5xzI8DDwJpx5zigyOIVFALHgMikq0nC7o5+vvyz7YRHo9Px9iIiGSuZQK8HWsdstyX2jfUtYClwGHgV+JRzLpaSCsdpOz7IA8/up/nA8el4exGRjJVMoE/U7x9/V4z3A5uBOcAK4FtmVvyWNzJba2bNZtbc1dU1yVLjLm+swO81nmmZ2utFJHv19PRw7733Tvp111xzDT09PZN+3R133MF//ud/Tvp10yWZQG8D5o7ZbiDeEx/rE8CPXVwLsB9YMv6NnHP3O+eanHNNVVUTLkVwRgVBH5fOL+OJre2MRqfllwARyVCnCvRo9PRDtBs2bKC0tHSaqpo5yUxbfBFYZGaNwCHgRuDmcee8BrwHeMbMaoDzgH2pLHSsP76ykbXffYnvPXeQT1yZ/JQeEZk5X/zpNrYf7kvpey6bU8zffPD8Ux5ft24de/fuZcWKFfj9fgoLC6mrq2Pz5s1s376dD3/4w7S2thIOh/nUpz7F2rVrgTfWlhoYGGD16tW84x3vYNOmTdTX1/PYY4+Rl5d3xto2btzIpz/9aSKRCJdddhn33XcfwWCQdevW8fjjj+Pz+Xjf+97H1772NX70ox/xxS9+Ea/XS0lJCU8//XRK2ueMge6ci5jZXcATgBd40Dm3zczuTBxfD/wd8B0ze5X4EM1nnXNHU1LhBN67rIZ3Lqrk67/czUcurqc0XxNqRAS++tWvsnXrVjZv3syTTz7Jtddey9atW0/O5X7wwQcpLy9naGiIyy67jOuvv56Kioo3vceePXv4wQ9+wLe//W1uuOEGHnnkEW699dbTfm44HOaOO+5g48aNLF68mNtuu4377ruP2267jUcffZSdO3diZieHdb70pS/xxBNPUF9fP6WhnlNJ6sIi59wGYMO4fevHPD8MvC9lVZ2BmfG51Uu55pvP8O8vvMb/vHrhTH20iCTpdD3pmbJy5co3XZjzzW9+k0cffRSA1tZW9uzZ85ZAb2xsZMWKFQBceumlHDhw4Iyfs2vXLhobG1m8eDEAt99+O/fccw933XUXoVCIP/3TP+Xaa6/lAx/4AABXXnkld9xxBzfccAPXXXddCn7SuIxdy2XZnGLetbiKu3+5h00t0/bLgIhksIKCgpPPn3zySX71q1/xu9/9ji1btnDxxRdPeOFOMBg8+dzr9RKJnHkGtnPj54nE+Xw+XnjhBa6//np+8pOfsGrVKgDWr1/Pl7/8ZVpbW1mxYgXd3d2T/dEmlLGBDvCNj62gIOjl0VcOpbsUEZkFioqK6O/vn/BYb28vZWVl5Ofns3PnTp577rmUfe6SJUs4cOAALS0tAHz3u9/lqquuYmBggN7eXq655hruvvtuNm/eDMDevXu5/PLL+dKXvkRlZSWtra2neffkZfRaLmUFAS6dX07zQc1JFxGoqKjgyiuv5IILLiAvL4+ampqTx1atWsX69eu56KKLOO+887jiiitS9rmhUIiHHnqIj370oye/FL3zzjs5duwYa9asIRwO45zj61//OgCf+cxn2LNnD8453vOe97B8+fKU1GGn+lVhujU1NblU3LHon5/ay9//fCfPfvbdNJTlp6AyEZmqHTt2sHTp0nSXkTUmak8ze8k51zTR+Rk95AJw7UV1+L3G+qf2prsUEZG0yvhAbyjL50PL63nslcMMR7S+i4ik3ic/+UlWrFjxpsdDDz2U7rLeIqPH0F/3gYvqeOTlNja1dPPuJdXpLkckpznnsm7FxXvuuWfGP3Mqw+EZ30MHePvC+Pouz+8/lu5SRHJaKBSiu7t7SmEkb3j9BhehUGhSr8uKHnrQ52VhdRE721N7mbGITE5DQwNtbW1MdfE9ecPrt6CbjKwIdICltUVs2puayfkiMjV+v39St0yT1MqKIReAJXVFtPeFOX5iJN2liIikRfYEem18+fWd7RNfJSYiku2yJ9DrigA0ji4iOStrAr2qMEhFQYCdR9RDF5HclDWBbmYsqdNMFxHJXVkT6ABLa4vZ1dFPNKY5sCKSe7Iq0JfUFRMejXGg+0S6SxERmXHZFei1iS9GNY4uIjkoqUA3s1VmtsvMWsxs3QTHP2NmmxOPrWYWNbPy1Jd7egurC/F6TOPoIpKTzhjoZuYF7gFWA8uAm8xs2dhznHP/5Jxb4ZxbAXwOeMo5N+MLq4T8Xs6pLGCHeugikoOS6aGvBFqcc/uccyPAw8Ca05x/E/CDVBQ3FUvqitVDF5GclEyg1wNjb3jXltj3FmaWD6wCHjnF8bVm1mxmzdO1eM+S2iLajg/RM6glAEQktyQT6BMtbHyqeYEfBH57quEW59z9zrkm51xTVVVVsjVOyhXnxIfun95zdFreX0Rktkom0NuAuWO2G4DDpzj3RtI43AKwYm4Z5QUBfr2jI51liIjMuGQC/UVgkZk1mlmAeGg/Pv4kMysBrgIeS22Jk+P1GJfOL2P7EY2ji0huOWOgO+ciwF3AE8AO4IfOuW1mdqeZ3Tnm1I8A/+2cS/tVPXPL8mk9NqS7pohITknqBhfOuQ3AhnH71o/b/g7wnVQVdjbmlecxNBql+8QIlYXBdJcjIjIjsupK0dfNLc8H4LVjg2muRERk5mR1oLcq0EUkh2RloM+vyCfg8/BqW2+6SxERmTFZGehBn5eL55by/P4ZX31ARCRtsjLQAS4/p4Jth3sZHImkuxQRkRmRtYF+TmUBMQdHesPpLkVEZEZkbaBXF8WnK3b2Dae5EhGRmZG9gV6cCPR+9dBFJDdkbaBXFYUA9dBFJHdkbaAXh3wEfR710EUkZ2RtoJsZNcUhOvvVQxeR3JC1gQ7QUJbHrnbdjk5EckNWB/ofLq1hZ3s/LZ0D6S5FRGTaZXWgr76wFoAnd3WmuRIRkemX1YFeV5JHbXGIrYe0pouIZL+sDnSAC+qL2XpYdy8SkeyX9YF+/pwS9nYNMDQSTXcpIiLTKqlAN7NVZrbLzFrMbN0pzrnazDab2TYzeyq1ZU7dvPJ8nIP2Ps1HF5HsdsZANzMvcA+wGlgG3GRmy8adUwrcC3zIOXc+8NHUlzo1tSXxK0bbtUiXiGS5ZHroK4EW59w+59wI8DCwZtw5NwM/ds69BuCcmzXTSmqK44HeoR66iGS5ZAK9Hmgds92W2DfWYqDMzJ40s5fM7LaJ3sjM1ppZs5k1d3V1Ta3iSTrZQ1egi0iWSybQbYJ9bty2D7gUuBZ4P/B5M1v8lhc5d79zrsk511RVVTXpYqeiMOijMOjTkIuIZD1fEue0AXPHbDcAhyc456hz7gRwwsyeBpYDu1NS5VmqKQ4q0EUk6yXTQ38RWGRmjWYWAG4EHh93zmPAO83MZ2b5wOXAjtSWOnVzy/NpPT6Y7jJERKbVGXvozrmImd0FPAF4gQedc9vM7M7E8fXOuR1m9gvg90AMeMA5t3U6C5+MBRUFvLj/GM45zCYaQRIRyXzJDLngnNsAbBi3b/247X8C/il1paVOY2UBJ0aidPUPU52Y9SIikm2y/kpRgAWVBQDsP3oizZWIiEyfnAj0xop4oB/oVqCLSPbKiUCfUxrC7zX2H9UXoyKSvXIi0H1eD/PK8zmgIRcRyWI5EegQ/2JUY+giks1yJtAXVBRwoPsEsdj4i1xFRLJD7gR6ZQHDkZjWdBGRrJUzgd6YmLqocXQRyVY5F+j7NXVRRLJUzgR6bXGIoM/D/i4Fuohkp5wJdI/HTn4xKiKSjXIm0AEWVOZr6qKIZK0cC/QCWo8NEdXURRHJQjkV6I0VBYxEYxzuGUp3KSIiKZdbgZ6Y6bJPwy4ikoVyKtDPrS4EYE9Hf5orERFJvZwK9MrCIJWFQXa2K9BFJPskFehmtsrMdplZi5mtm+D41WbWa2abE48vpL7U1FhaV8SOI33pLkNEJOXOGOhm5gXuAVYDy4CbzGzZBKc+45xbkXh8KcV1psyyumL2dAwQicbSXYqISEol00NfCbQ45/Y550aAh4E101vW9FlQGZ/p0tE/nO5SRERSKplArwdax2y3JfaN9zYz22JmPzez8yd6IzNba2bNZtbc1dU1hXLPXkNZHgCHjmvqoohkl2QC3SbYN/7KnJeB+c655cD/A34y0Rs55+53zjU555qqqqomVWiqNJTlA9B2XLejE5HskkygtwFzx2w3AIfHnuCc63PODSSebwD8ZlaZsipTaE5pCIA29dBFJMskE+gvAovMrNHMAsCNwONjTzCzWjOzxPOVifftTnWxqRD0eakpDtJ6TD10EckuvjOd4JyLmNldwBOAF3jQObfNzO5MHF8P/BHw52YWAYaAG51zs3bBlAUVBbpaVESyzhkDHU4Oo2wYt2/9mOffAr6V2tKmz5LaIh55+RCxmMPjmegrAhGRzJNTV4q+bkldMQPDEQ5pkS4RySI5Gejn1RYB6IpREckqORno51bFF+nSzS5EJJvkZKCX5PmpLAwo0EUkq+RkoINmuohI9snZQG+sLFAPXUSySs4G+oLKArr6hxkciaS7FBGRlMjZQNciXSKSbXI+0Ns0F11EskTOBnp96eurLirQRSQ75GygVxcF8XtNQy4ikjVyNtA9HqO+NE+rLopI1sjZQAdYWF3E7o7+dJchIpISOR3o59UWsu/oCYYj0XSXIiJy1nI80IuJxhx7O3WBkYhkvpwO9CWJVRd3dWjVRRHJfDkd6I2VBfi9xq72gXSXIiJy1pIKdDNbZWa7zKzFzNad5rzLzCxqZn+UuhKnj9/r4dyqQna1q4cuIpnvjIFuZl7gHmA1sAy4ycyWneK8fyB+79GMcV5tEbvaNdNFRDJfMj30lUCLc26fc24EeBhYM8F5/wt4BOhMYX3T7rzaIg73hukdGk13KSIiZyWZQK8HWsdstyX2nWRm9cBHgPVkmNe/GNV8dBHJdMkEuk2wz43bvhv4rHPutBO6zWytmTWbWXNXV1eSJU6vJbXFAOzU/UVFJMP5kjinDZg7ZrsBODzunCbgYTMDqASuMbOIc+4nY09yzt0P3A/Q1NQ0/j+FtKgrCVEc8rFD4+gikuGSCfQXgUVm1ggcAm4Ebh57gnOu8fXnZvYd4Gfjw3y2MjOWzy3l5YPH012KiMhZOeOQi3MuAtxFfPbKDuCHzrltZnanmd053QXOhJULytnV0U/P4Ei6SxERmbJkeug45zYAG8btm/ALUOfcHWdf1sy6rLEc5+CV13p495LqdJcjIjIlOX2l6OuWJr4Y1UwXEclkCnSgJN9PZWGQlk4tASAimUuBnrCoupCWLgW6iGQuBXrCwupCWjoGcG5WzKYUEZk0BXrCwupC+ocjdPYPp7sUEZEpUaAnLKwuBNA4uohkLAV6wqJEoO/RTBcRyVAK9ISqoiDFIR/btaaLiGQoBXqCmfEHS6r5+avtnBiOpLscEZFJU6CPcesV8+kfjvD4lvFrj4mIzH4K9DEunV/Gktoivv/8wXSXIiIyaQr0McyMay+sY+uhPvrDuoORiGQWBfo4S+vi67roPqMikmkU6OMsnRMP9B2a7SIiGUaBPs6cxB2Mth9RD11EMosCfRwzY2ldMTvb1UMXkcyiQJ/A0rpidrX3E4tpoS4RyRwK9AksqytmcCTKwWOD6S5FRCRpSQW6ma0ys11m1mJm6yY4vsbMfm9mm82s2czekfpSZ84l80sB+PXOzvQWIiIyCWcMdDPzAvcAq4FlwE1mtmzcaRuB5c65FcAfAw+kuM4ZtbC6iOVzS/lRc2u6SxERSVoyPfSVQItzbp9zbgR4GFgz9gTn3IB7484QBUDGDz5fe2EtO9v76ewPp7sUEZGkJBPo9cDYrmpbYt+bmNlHzGwn8F/Ee+lvYWZrE0MyzV1dXVOpd8asbKwA4MX9x9NciYhIcpIJdJtg31t64M65R51zS4APA3830Rs55+53zjU555qqqqomVehMO39OMfkBL7/bdzTdpYiIJCWZQG8D5o7ZbgBOuRyhc+5p4FwzqzzL2tLK7/XwrkVV/HJ7h6YvikhGSCbQXwQWmVmjmQWAG4HHx55gZgvNzBLPLwECQHeqi51pqy6opaNvmM1tPekuRUTkjHxnOsE5FzGzu4AnAC/woHNum5ndmTi+HrgeuM3MRoEh4GNjviTNWO9eUo3fazyxtZ1L5pWluxwRkdOydOVuU1OTa25uTstnT8btD75AS+cAT33manxeXYclIullZi8555omOqaEOoNbLp/HoZ4h3cVIRGY9BfoZvHdZDXPL8/j51vZ0lyIicloK9DMwM648t5IX9h/TbBcRmdUU6Em44pwKeodG2aEldUVkFlOgJ+Hyc8oBeG7fsTRXIiJyagr0JNSV5LGgIp/n9mX81HoRyWIK9CS9c1EVT+/u4kjvULpLERGZkAI9SWvfdQ7Owb2/2ZvuUkREJqRAT9Lc8nw+uHwOj7zcRn94NN3liIi8hQJ9Em65Yh6DI1Ge2NaR7lJERN5CgT4JF88tpb40jx+/3EYWLFUjIllGgT4JZsbtb5/Ppr3dfO/519JdjojImyjQJ+nP3nkOl84v41+e2acrR0VkVlGgT5KZcdvb5nOge5BnW3Q3IxGZPRToU7DqgloqCgJ897mD6S5FROQkBfoUBH1ebrhsLht3dHC4RxcaicjsoECfoptXzgPgL/5jM+HRaJqrERFJMtDNbJWZ7TKzFjNbN8HxW8zs94nHJjNbnvpSZ5e55fl85SMX8sL+YzyxTWuli0j6nTHQzcwL3AOsBpYBN5nZsnGn7Qeucs5dBPwdcH+qC52NPtY0l/rSPP7jxdZ0lyIiklQPfSXQ4pzb55wbAR4G1ow9wTm3yTl3PLH5HNCQ2jJnJ4/HuOPtC9i0t5uNO3T1qIikVzKBXg+M7YK2Jfadyp8AP5/ogJmtNbNmM2vu6upKvspZ7La3z+e8miL+8odb+K2mMYpIGiUT6DbBvgmvqDGzdxMP9M9OdNw5d79zrsk511RVVZV8lbNY0OflgdubqCwM8Ml/f5mB4Ui6SxKRHJVMoLcBc8dsNwCHx59kZhcBDwBrnHM5dSeIueX5fP1jK+gZHOWOB19gcEShLiIzL5lAfxFYZGaNZhYAbgQeH3uCmc0Dfgx83Dm3O/Vlzn4XNZTylY9cQPPB43zr1y3pLkdEctAZA905FwHuAp4AdgA/dM5tM7M7zezOxGlfACqAe81ss5k1T1vFs9gtl8/nuovr+fYz+9jXNZDuckQkx1i6loFtampyzc3Zl/ud/WHe8dXfcOsV8/nCB8fP7hQROTtm9pJzrmmiY7pSNMWqi0K8fWEFD/52P/c9qdvVicjMUaBPgw+viM/q/Idf7OQff7FTy+yKyIxQoE+DNSvm8PLn38sfLq3h3if38tPfv2VSkIhIyinQp4GZUV4Q4J8/finL6or528e38cvtupJURKaXAn0aeT3GvbdcQn7Ax5/9WzMPPLMv3SWJSBZToE+zBZUF/ObTV3PNhbV8+b92cOsDz2u5XRGZFgr0GRDwefjGjRdzx9sX8GzLUf7q0VcV6iKScr50F5Ar/F4Pf/PBZbQdH+THLx8iz+/l1ivms7imCK9nouVyREQmRz30GWRmfPu2Jj5+xXy+//xrrP7GM/zFf2xm444ORqOxdJcnIhlOPfQZZmb8zQeX0VCWxyMvt/HTLYf56ZbDfPp9i1mzop655fnpLlFEMpQu/U8j5xzfe/41Pv+TrSf33XfLJay+sC6NVYnIbHa6S//VQ08jM+PjV8znsgVl/OSVeE/9z7//Mn9+9bm8/dwK3rkoO9aMF5GZoR76LPKj5lbu/tUeDvUMAXD3x1YwryKfxooCygoCaa5ORGaD0/XQFeizUO/gKLc/9AKbW3sAqCgIcP9tTVw6vyy9hYlI2mm1xQxTku/n4bVX8PfXXcgl80rpPjHC9fdt4pYHnuNwzxCd/WH2dPSnu0wRmWXUQ5/lYjHH03u6eGH/Mb6z6QBBn4cTw1EisRjf/ZPLuXJhZbpLFJEZpCGXLPHKa8d54Jn9dA0M89LB45Tk+bntbfN51+Iqzqspwuc1IlFHQdCHcw4zXbAkkm3OOtDNbBXwDcALPOCc++q440uAh4BLgL92zn3tTO+pQD87O9v7WPtvL/HascE37S8K+vD7PFxYX8J3PnGZQl0ky5xVoJuZF9gNvBdoI37T6Jucc9vHnFMNzAc+DBxXoM8M5xzHB0f5xdZ2DvcMsbO9n6MDwye/TH3/+TUMjkRZs6Ke6y6ux6MlBkQy3tnOQ18JtDjn9iXe7GFgDXAy0J1znUCnmV2bgnolSa+vu37z5fPetH9wJMKnf7SFDa+2U10U5NM/2sJf/fhV3rW4kkU1RcRiDo/HKM8P8O4l1SysLkzTTyAiqZRMoNcDrWO224DLp/JhZrYWWAswb968M5wtU5Uf8HHvLZcyHIliGD/feoSndnfxm52dPLmri8iYW+J9ZcMOPnHlAi6eV0ZR0EdhyEfT/DIN1YhkoGQCfaJ/2VP6JtU5dz9wP8SHXKbyHpK8oM8LwJoV9axJ3Of0xHCEvvAoPo+HvV0D3P7gC/zrpgM89NsDb3rtOxdVcm5VIRc1lBDye6kqClJeEODcKvXmRWarZAK9DZg7ZrsB0E0yM1RB0EdBMP7XXlUU5NW/fT8ALZ0DDEeirH9qL09s62B3Rz/NB47znU0HTr7W6zEumVdKZWGQ4pCfisIAo9EYf7CkhnOrC6guCqXjRxKRhGQC/UVgkZk1AoeAG4Gbp7UqmTEBX/zasmVzigH41s2X0Nk/TH1pHsORKLvbB+gLjzISjbFxRwe72wfYuLMTj0F4NL7k77ef2Y/H4v9ZBH0e5pXn86Hlc3j1UB9eD1x3SQMdfWFizlEQ8HHVeVU8uauL3e39/FFTA3UleWn7+UWySbLTFq8B7iY+bfFB59xXzOxOAOfcejOrBZqBYiAGDADLnHN9p3pPzXLJXIMjEfxeD8dPjLC7Y4DtR3rpGRylo2+Y3qFRntvXzcBwhIqCAP3hCCPj1nqvKAjQfWIEiPf6L6wvYcXcUsoLAhSFfAyEI1x3aQP1pQp6kfF0YZHMqPBolK7+YRrK8jjcG+bZPV2UFwSpKgpypGeIzz+2lYqCIF+9/kI27ujkuX3dbD/Sx+DIG7flKwr5OH9OMV39w1QUBinJ8+Oc49L55YxGYyyuKaI8sWBZXUnoLevIR6IxfF6tbCHZR4Eus8qprmIdicToC4/S0RfmX57dz8sHj3NOVSED4Qgvv3b8TbNzxgv540M9fUMRzKCzf5iLGkqIxhy9Q6OsWT6HW6+Yz39v7yA8GiXk9/K+82uoKgxqRo9kFAW6ZLzDPUPk+b0MDEcI+j20dAzQ0R+mMOjnV9s72NnRT3m+n4DPw6aWbt52bgX7jp6gtjiEGTyz5+iE7xvwejivtoiOvjDVxUGGRqIsrSumLD9A1DmODYzQPzzKe5fW4IAFFQV4PIYR//6hoiDAopqiGW0LyW0KdMkpE/0GsKW1h407O1lWV8xwJMq5VYU8t6+bg92DbD3cS0NZPsdPjBBzjt0dA4xEogR8HoI+LydGIvQMjp7y8woTXwb7vMZo1DEaiRH0e6gtCVGWH6C2OETI72X1hbUMjUTJD/gYjcY41DPEebVFPPTbAyyuLuTdS6pZWlesm4bLaSnQRc5CLOY41DOE3+vhYPcJjg6MJMLew672fna29+MxGBqNUhTyEYtxcthnS2sPxwfjXwCfasTIY28cC3g9eDxwTmUho9EY0Zhjbnk+HoOD3YNUFgapL8sjEnM0VhbgNaOswE94NMrhnjDOOaqLQ1zUUMLSumIGwhGKQj62He5jSW0R1cWaWprpdAs6kbPg8djJL11rS94ciO9afPrbBPYMjjA4EsUMdncMUJLnp28o3tufV57Psy1HaSjL49yqQl46eJztR/oYGI6wt3OA8oIAZrDjSD9HeocIeD0UhXw8s6cLjxk/3fLmy0EKE6tsnhjz5fJY+QEvZfkBhkajLKwuxGPxWUZ+r4cTwxHKCwLMKc0jFnNEYo7BkSh5AS8eg/KCIMsbSqgpDtHSOYBZ/IrkupIQXf3DlBcEyA94Ty4j0d4XJs/vpSjkZzQaIzwapXdolLKCAMUh/5T+HuTM1EMXyQDDkSg+j+dNwzGHeoYozw9wdGCYknw/xaH4TKDBkSi/bTnKkd4wQZ+HgeEIfeEI+7oGGBiOUJ4foK1nCBxEYrHEFNMgh3qGOJaYTmoGOBiJxnDAaDRGMlFRFPIxNBI9+QV2wOdhJPLGtFWvx6gtDlEQ9FIQ9FFfmkdL5wDVxSEqCwNcWF9CRWGQ9t4h8gI+SvP8lBcE2NzaQzTmWFJbxOBIlBMjERZWFfLqoV5WXVBLf+I3kdGoY155PtGYY09nP4HETKeF1YWYGQPDEfL83owe1tKQi4hMWiwRyh6P0Ts0yrbDvfQNRagtCZ38j+KV146zuKaIkUiM9r4wz+8/RkVB4GSoth0fojTfT0men4Kgj31dA+zrOkHXwDCRqGMoMeOoPzxKeDTG0YHhs647P+BlOBIfrnpdQcBLwOehZ2iU6qIg9aV5J6e1GnB8cITakjzCo1FqikP4PEbP4Ag9Q6OcP6eY82qK6B+O4Pd4TraJc44jvWGKQj4uW1DOsRMjDAxHmF+Rz/BofMbW67/1lOT5qSgIYGZ09IUxmPLwlwJdRDJCR1+Y7oERKosCRGOO/nCEzr5hltQVkef3srujn5A/vkbRk7u6uPyccjbu6GBOaR49g6OU5QfYdriXsvwAi2rivfL+8Ch7OgaIxhwBn4cjvUP0Do0SjbmTv3UEfB7ae8OMRGMc7B5kTkmI/KCPysIAr7b1nnIYK+T3MBxJ7reXgNcDFp+e+z+uOofPrV46pTbSGLqIZISa4hA1Y3qudSWweMy00IvnvXGj9KV18eUqLhmz72y9/h1EYfCNaIxEYyd74hAfNnq9818Y9HF0YJh9XScoLwjg8xqHjg8R8nsZjcYYicboGxql9dgg7X1hCgI+ikI+rrmwLmU1j6VAFxFJMLM3hTmAz+t5y5XIY43/TyidK5Lq2mgRkSyhQBcRyRIKdBGRLKFAFxHJEgp0EZEsoUAXEckSCnQRkSyhQBcRyRJpu/TfzLqAg1N8eSUw8R0LcovaIU7tEKd2eEM2t8V859yEy3ymLdDPhpk1n2otg1yidohTO8SpHd6Qq22hIRcRkSyhQBcRyRKZGuj3p7uAWULtEKd2iFM7vCEn2yIjx9BFROStMrWHLiIi4yjQRUSyRMYFupmtMrNdZtZiZuvSXc90MrMHzazTzLaO2VduZr80sz2JP8vGHPtcol12mdn701N16pnZXDP7jZntMLNtZvapxP6cagszC5nZC2a2JdEOX0zsz6l2ADAzr5m9YmY/S2znXBtMyDmXMQ/AC+wFzgECwBZgWbrrmsaf913AJcDWMfv+EViXeL4O+IfE82WJ9ggCjYl28qb7Z0hRO9QBlySeFwG7Ez9vTrUF8fsZFyae+4HngStyrR0SP9tfAv8O/CyxnXNtMNEj03roK4EW59w+59wI8DCwJs01TRvn3NPAsXG71wD/mnj+r8CHx+x/2Dk37JzbD7QQb6+M55w74px7OfG8H9gB1JNjbeHiBhKb/sTDkWPtYGYNwLXAA2N251QbnEqmBXo90Dpmuy2xL5fUOOeOQDzogOrE/pxoGzNbAFxMvHeac22RGGrYDHQCv3TO5WI73A38byA2Zl+utcGEMi3QbYJ9mncZl/VtY2aFwCPAXzjn+k536gT7sqItnHNR59wKoAFYaWYXnOb0rGsHM/sA0OmceynZl0ywL6Pb4HQyLdDbgLljthuAw2mqJV06zKwOIPFnZ2J/VreNmfmJh/n3nXM/TuzOybYAcM71AE8Cq8itdrgS+JCZHSA+5PoHZvY9cqsNTinTAv1FYJGZNZpZALgReDzNNc20x4HbE89vBx4bs/9GMwuaWSOwCHghDfWlnJkZ8C/ADufc/x1zKKfawsyqzKw08TwP+ENgJznUDs65zznnGpxzC4j/+/+1c+5WcqgNTivd38pO9gFcQ3yWw17gr9NdzzT/rD8AjgCjxHsafwJUABuBPYk/y8ec/9eJdtkFrE53/Slsh3cQ/zX598DmxOOaXGsL4CLglUQ7bAW+kNifU+0w5me7mjdmueRkG4x/6NJ/EZEskWlDLiIicgoKdBGRLKFAFxHJEgp0EZEsoUAXEckSCnQRkSyhQBcRyRL/H2HOY9s+981NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.legend(['train_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction du tirage suivant le dernier tirage de notre dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\free\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Prediction basée sur les 12 derniers tirages\n",
    "last_twelve = df.tail(window_length) # on recupere les 12 derniers tirages\n",
    "scaler = StandardScaler().fit(df.values)\n",
    "scaled_to_predict = scaler.transform(last_twelve)\n",
    "scaled_predicted_output_1 = model.predict(np.array([scaled_to_predict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  7  9 16 15 19 25 30 31 35 35 37 40 43 44 51 55 59 63 66]\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "tom = df.tail(window_length).iloc[:,0:20] # \n",
    "scaler = StandardScaler().fit(df.iloc[:,0:20])\n",
    "scaled_to_predict = scaler.transform(tom)\n",
    "print(scaler.inverse_transform(scaled_predicted_output_1).astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
